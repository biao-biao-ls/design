# ğŸ“— KNZN Docker å…¨æ ˆéƒ¨ç½²ä¸è¿ç»´æŒ‡å— (Contabo VPS)

> **æ ¸å¿ƒç†å¿µ**: å•æœºå®¹å™¨åŒ–é›†ç¾¤ - Docker Compose ç¼–æ’ + Nginx åä»£ + è‡ªåŠ¨åŒ– CI/CD

## ğŸ“‹ æ–‡æ¡£æ¦‚è¿°

**éƒ¨ç½²ç­–ç•¥**: Contabo VPS å•æœºå®¹å™¨åŒ–é›†ç¾¤  
**ç¡¬ä»¶é…ç½®**: Contabo VPS L (12GB RAM, 6 CPU cores, 100GB NVMe)  
**è¿ç»´ç†å¿µ**: è‡ªåŠ¨åŒ–éƒ¨ç½²ï¼Œæœ€å°åŒ–äººå·¥å¹²é¢„  
**æˆæœ¬æ§åˆ¶**: æœˆè¿è¥æˆæœ¬ < $50ï¼Œæ”¯æ’‘ 10K+ ç”¨æˆ·  
**æ–‡æ¡£ç‰ˆæœ¬**: v2.0 (Contabo VPS ä¸“ç”¨ç‰ˆ)  

### ğŸ–¥ï¸ æ¨èç¡¬ä»¶é…ç½®

| é…ç½®é¡¹ | è§„æ ¼ | è¯´æ˜ |
|--------|------|------|
| **CPU** | 6 vCPU cores | æ”¯æŒå¹¶è¡Œå¤„ç†å’Œå¤šå®¹å™¨è¿è¡Œ |
| **å†…å­˜** | 12GB RAM | PostgreSQL (6GB) + Nuxt App (4GB) + Redis (1GB) + ç³»ç»Ÿ (1GB) |
| **å­˜å‚¨** | 100GB NVMe | é«˜é€Ÿ SSDï¼Œä¼˜åŒ–æ•°æ®åº“ I/O æ€§èƒ½ |
| **ç½‘ç»œ** | ä¸é™æµé‡ | æ”¯æŒå…¨çƒç”¨æˆ·è®¿é—® |
| **æœˆè´¹ç”¨** | ~$13 USD | æ€§ä»·æ¯”æé«˜çš„é…ç½®é€‰æ‹© |  

## ğŸ—ï¸ Docker å®¹å™¨åŒ–æ¶æ„

### æ•´ä½“éƒ¨ç½²æ‹“æ‰‘

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        å…¨çƒç”¨æˆ·è®¿é—®                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸŒ Global Users â”‚ ğŸ‡ºğŸ‡¸ US â”‚ ğŸ‡ªğŸ‡º EU â”‚ ğŸ‡¦ğŸ‡º AU â”‚ ğŸ‡¯ğŸ‡µ JP          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Cloudflare (å¯é€‰ CDN)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ å…¨çƒ CDN åŠ é€Ÿ (é™æ€èµ„æº)                                      â”‚
â”‚ â€¢ DDoS é˜²æŠ¤                                                     â”‚
â”‚ â€¢ DNS ç®¡ç†                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Contabo VPS (å•æœºé›†ç¾¤)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     Nginx å®¹å™¨ (å…¥å£)                          â”‚
â”‚ â€¢ SSL è¯ä¹¦ç®¡ç† (Let's Encrypt)                                 â”‚
â”‚ â€¢ HTTP/2 + Gzip å‹ç¼©                                           â”‚
â”‚ â€¢ é™æ€èµ„æºç¼“å­˜ (æ›¿ä»£ Vercel Edge åŠŸèƒ½)                        â”‚
â”‚ â€¢ åå‘ä»£ç†åˆ° Nuxt å®¹å™¨                                          â”‚
â”‚ â€¢ å®‰å…¨å¤´é…ç½®                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Nuxt 4 å®¹å™¨     â”‚ â”‚ PostgreSQL å®¹å™¨  â”‚ â”‚   Redis å®¹å™¨     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Vue 3 å‰ç«¯     â”‚ â”‚ â€¢ ä¸»æ•°æ®åº“       â”‚ â”‚ â€¢ Nitro ç¼“å­˜     â”‚
â”‚ â€¢ Nitro åç«¯     â”‚ â”‚ â€¢ æ•°æ®æŒä¹…åŒ–     â”‚ â”‚ â€¢ ä¼šè¯å­˜å‚¨       â”‚
â”‚ â€¢ Better-Auth    â”‚ â”‚ â€¢ è‡ªåŠ¨å¤‡ä»½       â”‚ â”‚ â€¢ é™æµæ§åˆ¶       â”‚
â”‚ â€¢ Drizzle ORM    â”‚ â”‚ â€¢ æ€§èƒ½ä¼˜åŒ–       â”‚ â”‚ â€¢ æ’è¡Œæ¦œç¼“å­˜     â”‚
â”‚ â€¢ ç«¯å£: 3000     â”‚ â”‚ â€¢ ç«¯å£: 5432     â”‚ â”‚ â€¢ ç«¯å£: 6379     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Docker é…ç½®æ–‡ä»¶ç»“æ„

```
knzn-project/
â”œâ”€â”€ docker-compose.yml              # ä¸»ç¼–æ’æ–‡ä»¶
â”œâ”€â”€ docker-compose.prod.yml         # ç”Ÿäº§ç¯å¢ƒè¦†ç›–
â”œâ”€â”€ .env.production                 # ç”Ÿäº§ç¯å¢ƒå˜é‡
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ nginx/
â”‚   â”‚   â”œâ”€â”€ Dockerfile              # Nginx å®¹å™¨
â”‚   â”‚   â”œâ”€â”€ nginx.conf              # ä¸»é…ç½®
â”‚   â”‚   â”œâ”€â”€ ssl.conf                # SSL é…ç½®
â”‚   â”‚   â””â”€â”€ cache.conf              # ç¼“å­˜é…ç½®
â”‚   â”‚
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ Dockerfile              # Nuxt åº”ç”¨å®¹å™¨
â”‚   â”‚   â””â”€â”€ .dockerignore           # æ„å»ºå¿½ç•¥æ–‡ä»¶
â”‚   â”‚
â”‚   â””â”€â”€ postgres/
â”‚       â”œâ”€â”€ init.sql                # åˆå§‹åŒ–è„šæœ¬
â”‚       â””â”€â”€ postgresql.conf         # æ€§èƒ½ä¼˜åŒ–é…ç½®
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ deploy.sh                   # éƒ¨ç½²è„šæœ¬
â”‚   â”œâ”€â”€ backup.sh                   # å¤‡ä»½è„šæœ¬
â”‚   â”œâ”€â”€ restore.sh                  # æ¢å¤è„šæœ¬
â”‚   â””â”€â”€ update.sh                   # æ›´æ–°è„šæœ¬
â”‚
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ deploy.yml              # GitHub Actions CI/CD
```

## ğŸ³ Docker Compose é…ç½®

### ä¸»ç¼–æ’æ–‡ä»¶

```yaml
# docker-compose.yml
version: '3.8'

services:
  # ğŸŒ Nginx åå‘ä»£ç†
  nginx:
    build:
      context: ./docker/nginx
      dockerfile: Dockerfile
    container_name: knzn-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./docker/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./docker/nginx/ssl.conf:/etc/nginx/conf.d/ssl.conf:ro
      - ./docker/nginx/cache.conf:/etc/nginx/conf.d/cache.conf:ro
      - nginx_cache:/var/cache/nginx
      - certbot_certs:/etc/letsencrypt
      - certbot_www:/var/www/certbot
    depends_on:
      - app
    restart: unless-stopped
    networks:
      - knzn-network

  # ğŸš€ Nuxt 4 åº”ç”¨
  app:
    build:
      context: .
      dockerfile: ./docker/app/Dockerfile
      target: production
    container_name: knzn-app
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - NITRO_PORT=3000
      - NITRO_HOST=0.0.0.0
      - DATABASE_URL=postgresql://knzn_user:${DATABASE_PASSWORD}@postgres:5432/knzn_production
      - REDIS_URL=redis://redis:6379
    env_file:
      - .env.production
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    restart: unless-stopped
    networks:
      - knzn-network
    # ğŸ”’ å®‰å…¨é…ç½®
    security_opt:
      - no-new-privileges:true
    # ğŸ“Š èµ„æºé™åˆ¶ (Contabo ä¼˜åŒ–)
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'

  # ğŸ—„ï¸ PostgreSQL æ•°æ®åº“
  postgres:
    image: postgres:15-alpine
    container_name: knzn-postgres
    environment:
      POSTGRES_DB: knzn_production
      POSTGRES_USER: knzn_user
      POSTGRES_PASSWORD: ${DATABASE_PASSWORD}
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --lc-collate=C --lc-ctype=C"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
      - ./docker/postgres/postgresql.conf:/etc/postgresql/postgresql.conf:ro
    ports:
      - "5432:5432"  # ä»…ç”¨äºå¤‡ä»½ï¼Œç”Ÿäº§ç¯å¢ƒå¯å…³é—­
    restart: unless-stopped
    networks:
      - knzn-network
    # ğŸ¥ å¥åº·æ£€æŸ¥
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U knzn_user -d knzn_production"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    # ğŸ“Š èµ„æºé™åˆ¶
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  # ğŸ”„ Redis ç¼“å­˜
  redis:
    image: redis:7-alpine
    container_name: knzn-redis
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"  # ä»…ç”¨äºè°ƒè¯•ï¼Œç”Ÿäº§ç¯å¢ƒå¯å…³é—­
    restart: unless-stopped
    networks:
      - knzn-network
    # ğŸ“Š èµ„æºé™åˆ¶
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'

  # ğŸ”’ SSL è¯ä¹¦ç®¡ç†
  certbot:
    image: certbot/certbot
    container_name: knzn-certbot
    volumes:
      - certbot_certs:/etc/letsencrypt
      - certbot_www:/var/www/certbot
    command: certonly --webroot --webroot-path=/var/www/certbot --email admin@knzn.net --agree-tos --no-eff-email -d knzn.net -d www.knzn.net
    depends_on:
      - nginx
    profiles:
      - ssl-init  # ä»…åœ¨åˆå§‹åŒ–æ—¶è¿è¡Œ

# ğŸ“¦ æ•°æ®å·
volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  nginx_cache:
    driver: local
  certbot_certs:
    driver: local
  certbot_www:
    driver: local

# ğŸŒ ç½‘ç»œ
networks:
  knzn-network:
    driver: bridge
```

### ç”Ÿäº§ç¯å¢ƒè¦†ç›–é…ç½®

```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  nginx:
    # ğŸ”’ ç”Ÿäº§ç¯å¢ƒå®‰å…¨åŠ å›º
    security_opt:
      - no-new-privileges:true
    # ğŸ“Š æ—¥å¿—é…ç½®
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  app:
    # ğŸš€ ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–
    environment:
      - NODE_ENV=production
      - NUXT_TELEMETRY_DISABLED=1
    # ğŸ“Š æ—¥å¿—é…ç½®
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  postgres:
    # ğŸ”’ ç”Ÿäº§ç¯å¢ƒç«¯å£å…³é—­
    ports: []
    # ğŸ“Š æ—¥å¿—é…ç½®
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "3"

  redis:
    # ğŸ”’ ç”Ÿäº§ç¯å¢ƒç«¯å£å…³é—­
    ports: []
    # ğŸ“Š æ—¥å¿—é…ç½®
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "3"
```

## ğŸ³ Dockerfile é…ç½®

### Nuxt 4 åº”ç”¨ Dockerfile

```dockerfile
# docker/app/Dockerfile
# å¤šé˜¶æ®µæ„å»ºï¼šBuild Stage -> Production Stage

# ==========================================
# Build Stage (æ„å»ºé˜¶æ®µ)
# ==========================================
FROM node:20-alpine AS builder

# ğŸ”§ å®‰è£…æ„å»ºä¾èµ–
RUN apk add --no-cache libc6-compat

# ğŸ“ è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# ğŸ“¦ å¤åˆ¶ package æ–‡ä»¶
COPY package.json pnpm-lock.yaml ./

# ğŸš€ å®‰è£… pnpm
RUN npm install -g pnpm

# ğŸ“¥ å®‰è£…ä¾èµ–
RUN pnpm install --frozen-lockfile

# ğŸ“‹ å¤åˆ¶æºä»£ç 
COPY . .

# ğŸ—ï¸ æ„å»ºåº”ç”¨
RUN pnpm run build

# ==========================================
# Production Stage (ç”Ÿäº§é˜¶æ®µ)
# ==========================================
FROM node:20-alpine AS production

# ğŸ”§ å®‰è£…è¿è¡Œæ—¶ä¾èµ–
RUN apk add --no-cache \
    dumb-init \
    curl \
    && addgroup -g 1001 -S nodejs \
    && adduser -S nuxt -u 1001

# ğŸ“ è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# ğŸ‘¤ åˆ‡æ¢åˆ°é root ç”¨æˆ·
USER nuxt

# ğŸ“‹ ä»æ„å»ºé˜¶æ®µå¤åˆ¶æ–‡ä»¶
COPY --from=builder --chown=nuxt:nodejs /app/.output /app/.output
COPY --from=builder --chown=nuxt:nodejs /app/package.json /app/package.json

# ğŸŒ æš´éœ²ç«¯å£
EXPOSE 3000

# ğŸ”’ è®¾ç½®ç¯å¢ƒå˜é‡
ENV NODE_ENV=production
ENV NITRO_PORT=3000
ENV NITRO_HOST=0.0.0.0

# ğŸ¥ å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:3000/api/health || exit 1

# ğŸš€ å¯åŠ¨åº”ç”¨
ENTRYPOINT ["dumb-init", "--"]
CMD ["node", ".output/server/index.mjs"]
```

### Nginx Dockerfile

```dockerfile
# docker/nginx/Dockerfile
FROM nginx:1.25-alpine

# ğŸ”§ å®‰è£…å¿…è¦å·¥å…·
RUN apk add --no-cache \
    curl \
    openssl

# ğŸ“‹ å¤åˆ¶é…ç½®æ–‡ä»¶
COPY nginx.conf /etc/nginx/nginx.conf
COPY ssl.conf /etc/nginx/conf.d/ssl.conf
COPY cache.conf /etc/nginx/conf.d/cache.conf

# ğŸ“ åˆ›å»ºå¿…è¦ç›®å½•
RUN mkdir -p /var/cache/nginx/client_temp \
    && mkdir -p /var/cache/nginx/proxy_temp \
    && mkdir -p /var/cache/nginx/fastcgi_temp \
    && mkdir -p /var/cache/nginx/uwsgi_temp \
    && mkdir -p /var/cache/nginx/scgi_temp

# ğŸ”’ è®¾ç½®æƒé™
RUN chown -R nginx:nginx /var/cache/nginx

# ğŸŒ æš´éœ²ç«¯å£
EXPOSE 80 443

# ğŸ¥ å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD curl -f http://localhost/health || exit 1

# ğŸš€ å¯åŠ¨ Nginx
CMD ["nginx", "-g", "daemon off;"]
```

## ğŸŒ Nginx é…ç½®

### ä¸»é…ç½®æ–‡ä»¶

```nginx
# docker/nginx/nginx.conf
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log notice;
pid /var/run/nginx.pid;

# ğŸš€ æ€§èƒ½ä¼˜åŒ–
worker_rlimit_nofile 65535;

events {
    worker_connections 4096;
    use epoll;
    multi_accept on;
}

http {
    # ğŸ“‹ åŸºç¡€é…ç½®
    include /etc/nginx/mime.types;
    default_type application/octet-stream;
    
    # ğŸ“Š æ—¥å¿—æ ¼å¼
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    'rt=$request_time uct="$upstream_connect_time" '
                    'uht="$upstream_header_time" urt="$upstream_response_time"';
    
    access_log /var/log/nginx/access.log main;
    
    # ğŸš€ æ€§èƒ½ä¼˜åŒ–
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    client_max_body_size 10M;
    
    # ğŸ—œï¸ Gzip å‹ç¼©
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/json
        application/javascript
        application/xml+rss
        application/atom+xml
        image/svg+xml;
    
    # ğŸ”’ å®‰å…¨å¤´
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header Referrer-Policy "no-referrer-when-downgrade" always;
    add_header Content-Security-Policy "default-src 'self' http: https: data: blob: 'unsafe-inline'" always;
    
    # ğŸ”„ ä¸Šæ¸¸æœåŠ¡å™¨
    upstream nuxt_app {
        server app:3000;
        keepalive 32;
    }
    
    # ğŸŒ ä¸»æœåŠ¡å™¨é…ç½®
    server {
        listen 80;
        server_name knzn.net www.knzn.net;
        
        # ğŸ”’ Let's Encrypt éªŒè¯
        location /.well-known/acme-challenge/ {
            root /var/www/certbot;
        }
        
        # ğŸ”„ é‡å®šå‘åˆ° HTTPS
        location / {
            return 301 https://$server_name$request_uri;
        }
    }
    
    # ğŸ”’ HTTPS æœåŠ¡å™¨é…ç½®
    server {
        listen 443 ssl http2;
        server_name knzn.net www.knzn.net;
        
        # ğŸ” SSL è¯ä¹¦
        ssl_certificate /etc/letsencrypt/live/knzn.net/fullchain.pem;
        ssl_certificate_key /etc/letsencrypt/live/knzn.net/privkey.pem;
        
        # ğŸ”’ SSL é…ç½®
        include /etc/nginx/conf.d/ssl.conf;
        
        # ğŸ“ é™æ€æ–‡ä»¶ç¼“å­˜
        include /etc/nginx/conf.d/cache.conf;
        
        # ğŸ¥ å¥åº·æ£€æŸ¥
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
        
        # ğŸš€ ä»£ç†åˆ° Nuxt åº”ç”¨
        location / {
            proxy_pass http://nuxt_app;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection 'upgrade';
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_cache_bypass $http_upgrade;
            
            # â±ï¸ è¶…æ—¶é…ç½®
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
            
            # ğŸ“Š ç¼“å†²é…ç½®
            proxy_buffering on;
            proxy_buffer_size 128k;
            proxy_buffers 4 256k;
            proxy_busy_buffers_size 256k;
        }
    }
}
```

## ğŸš€ Docker å®¹å™¨åŒ–éƒ¨ç½²é…ç½®

### Docker Compose é…ç½®æ–‡ä»¶

```yaml
# docker-compose.yml
version: '3.8'

services:
  # Nginx åå‘ä»£ç†
  nginx:
    image: nginx:alpine
    container_name: knzn-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - certbot_certs:/etc/letsencrypt
    depends_on:
      - app
    restart: unless-stopped
    networks:
      - knzn-network

  # Nuxt 4 åº”ç”¨
  app:
    image: ghcr.io/your-username/knzn-app:latest
    container_name: knzn-app
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - DATABASE_URL=postgresql://knzn_user:${DATABASE_PASSWORD}@postgres:5432/knzn_production
      - REDIS_URL=redis://redis:6379
    env_file:
      - .env.production
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    networks:
      - knzn-network

  # PostgreSQL æ•°æ®åº“
  postgres:
    image: postgres:15-alpine
    container_name: knzn-postgres
    environment:
      POSTGRES_DB: knzn_production
      POSTGRES_USER: knzn_user
      POSTGRES_PASSWORD: ${DATABASE_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 6G
          cpus: '3.0'
        reservations:
          memory: 3G
          cpus: '2.0'
    networks:
      - knzn-network

  # Redis ç¼“å­˜
  redis:
    image: redis:7-alpine
    container_name: knzn-redis
    command: redis-server --appendonly yes --maxmemory 1gb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'
    networks:
      - knzn-network

volumes:
  postgres_data:
  redis_data:
  certbot_certs:

networks:
  knzn-network:
    driver: bridge
```

### Dockerfile é…ç½®

```dockerfile
# Dockerfile
FROM node:18-alpine AS base

# å®‰è£…ä¾èµ–é˜¶æ®µ
FROM base AS deps
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

# æ„å»ºé˜¶æ®µ
FROM base AS builder
WORKDIR /app
COPY package*.json ./
RUN npm ci
COPY . .
RUN npm run build

# è¿è¡Œé˜¶æ®µ
FROM base AS runner
WORKDIR /app

ENV NODE_ENV=production

RUN addgroup --system --gid 1001 nodejs
RUN adduser --system --uid 1001 nuxtjs

COPY --from=deps /app/node_modules ./node_modules
COPY --from=builder /app/.output ./.output

USER nuxtjs

EXPOSE 3000

ENV PORT 3000
ENV HOSTNAME "0.0.0.0"

CMD ["node", ".output/server/index.mjs"]
```

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# .env.production
# ğŸ—„ï¸ æ•°æ®åº“é…ç½®
DATABASE_URL="postgresql://knzn_user:password@your-vps-ip:5432/knzn_production"
DATABASE_HOST="your-vps-ip"
DATABASE_NAME="knzn_production"
DATABASE_USER="knzn_user"
DATABASE_PASSWORD="your_secure_password"

# ğŸ” è®¤è¯æœåŠ¡
GOOGLE_CLIENT_ID="your_google_client_id"
GOOGLE_CLIENT_SECRET="your_google_client_secret"
GITHUB_CLIENT_ID="your_github_client_id"
GITHUB_CLIENT_SECRET="your_github_client_secret"
BETTER_AUTH_SECRET="your_auth_secret_key"

# ğŸ“§ é‚®ä»¶æœåŠ¡
RESEND_API_KEY="re_your_resend_api_key"

# ğŸ¤– AI æœåŠ¡
OPENAI_API_KEY="sk-your_openai_api_key"

# ğŸ“ æ–‡ä»¶å­˜å‚¨
R2_ACCESS_KEY_ID="your_r2_access_key"
R2_SECRET_ACCESS_KEY="your_r2_secret_key"
CLOUDFLARE_ACCOUNT_ID="your_cloudflare_account_id"
R2_BUCKET_NAME="knzn-assets"

# ğŸ’³ æ”¯ä»˜æœåŠ¡
LEMON_SQUEEZY_API_KEY="your_lemon_squeezy_api_key"
LEMON_SQUEEZY_WEBHOOK_SECRET="your_webhook_secret"

# ğŸ”’ å®‰å…¨é…ç½®
INTERNAL_API_KEY="your_internal_api_key"
BACKUP_ENCRYPTION_KEY="your_backup_encryption_key"
JWT_SECRET="your_jwt_secret"

# ğŸŒ ç«™ç‚¹é…ç½®
SITE_URL="https://knzn.net"
COOKIE_DOMAIN="knzn.net"
```

### éƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# deploy.sh - ä¸€é”®éƒ¨ç½²è„šæœ¬

echo "ğŸš€ Starting KNZN deployment..."

# 1. ç¯å¢ƒæ£€æŸ¥
if [ ! -f ".env.production" ]; then
    echo "âŒ .env.production file not found!"
    exit 1
fi

# 2. ä¾èµ–å®‰è£…
echo "ğŸ“¦ Installing dependencies..."
pnpm install --frozen-lockfile

# 3. ç±»å‹æ£€æŸ¥
echo "ğŸ” Type checking..."
pnpm run type-check

# 4. æ„å»ºæ£€æŸ¥
echo "ğŸ—ï¸ Building application..."
pnpm run build

# 5. æ•°æ®åº“è¿ç§» (ç”Ÿäº§ç¯å¢ƒ)
echo "ğŸ—„ï¸ Running database migrations..."
pnpm run db:migrate:prod

# 6. æ„å»ºå¹¶æ¨é€ Docker é•œåƒ
echo "ğŸ³ Building and pushing Docker image..."
docker build -t knzn-app:latest .
docker tag knzn-app:latest ghcr.io/your-username/knzn-app:latest
docker push ghcr.io/your-username/knzn-app:latest

# 7. éƒ¨ç½²åˆ° Contabo VPS
echo "ğŸš€ Deploying to Contabo VPS..."
ssh user@your-vps-ip "cd /opt/knzn && docker-compose pull && docker-compose up -d --force-recreate"

# 8. éƒ¨ç½²åéªŒè¯
echo "âœ… Verifying deployment..."
curl -f https://knzn.net/api/health || {
    echo "âŒ Health check failed!"
    exit 1
}

echo "ğŸ‰ Deployment completed successfully!"
echo "ğŸŒ Site: https://knzn.net"
echo "ğŸ“Š Dashboard: ssh user@your-vps-ip 'docker-compose logs -f'"
```

## ğŸ–¥ï¸ VPS æœåŠ¡å™¨é…ç½®

### æœåŠ¡å™¨åˆå§‹åŒ–è„šæœ¬

```bash
#!/bin/bash
# vps-setup.sh - VPS åˆå§‹åŒ–è„šæœ¬

echo "ğŸ–¥ï¸ Setting up KNZN VPS server..."

# 1. ç³»ç»Ÿæ›´æ–°
sudo apt update && sudo apt upgrade -y

# 2. å®‰è£…å¿…è¦è½¯ä»¶
sudo apt install -y \
    postgresql-14 \
    postgresql-contrib \
    redis-server \
    nginx \
    certbot \
    python3-certbot-nginx \
    ufw \
    fail2ban \
    htop \
    curl \
    wget \
    git

# 3. é…ç½®é˜²ç«å¢™
sudo ufw default deny incoming
sudo ufw default allow outgoing
sudo ufw allow ssh
sudo ufw allow 80
sudo ufw allow 443
sudo ufw allow 22   # SSH è®¿é—®
sudo ufw allow 6379  # Redis (æœ¬åœ°)
sudo ufw --force enable

# 4. é…ç½® PostgreSQL
sudo -u postgres psql << EOF
CREATE DATABASE knzn_production;
CREATE USER knzn_user WITH ENCRYPTED PASSWORD '$DATABASE_PASSWORD';
GRANT ALL PRIVILEGES ON DATABASE knzn_production TO knzn_user;
ALTER USER knzn_user CREATEDB;
\q
EOF

# 5. é…ç½® PostgreSQL è¿œç¨‹è®¿é—® (é’ˆå¯¹ 12GB RAM + 6 CPU cores ä¼˜åŒ–)
sudo tee -a /etc/postgresql/14/main/postgresql.conf << EOF
# KNZN Configuration - é’ˆå¯¹ Contabo VPS L (12GB RAM, 6 CPU cores, 100GB NVMe) ä¼˜åŒ–
listen_addresses = '*'
max_connections = 200
shared_buffers = 3GB                    # çº¦ 25% çš„ RAM
effective_cache_size = 8GB              # çº¦ 67% çš„ RAM
work_mem = 16MB                         # é€‚åˆé«˜å¹¶å‘
maintenance_work_mem = 512MB            # ç»´æŠ¤æ“ä½œå†…å­˜
checkpoint_completion_target = 0.9      # å¹³æ»‘æ£€æŸ¥ç‚¹
wal_buffers = 16MB                      # WAL ç¼“å†²åŒº
default_statistics_target = 100         # ç»Ÿè®¡ä¿¡æ¯ç²¾åº¦
random_page_cost = 1.1                  # NVMe SSD ä¼˜åŒ–
effective_io_concurrency = 200          # 6 CPU cores å¹¶å‘ä¼˜åŒ–
max_worker_processes = 6                # åŒ¹é… CPU æ ¸å¿ƒæ•°
max_parallel_workers = 4                # å¹¶è¡ŒæŸ¥è¯¢å·¥ä½œè¿›ç¨‹
max_parallel_workers_per_gather = 2     # æ¯ä¸ªæŸ¥è¯¢çš„å¹¶è¡Œå·¥ä½œè¿›ç¨‹
EOF

# 6. é…ç½® PostgreSQL è®¿é—®æ§åˆ¶
sudo tee -a /etc/postgresql/14/main/pg_hba.conf << EOF
# KNZN Docker å®¹å™¨è®¿é—®
host knzn_production knzn_user 172.18.0.0/16 md5
EOF

# 7. é‡å¯æœåŠ¡
sudo systemctl restart postgresql
sudo systemctl enable postgresql
sudo systemctl restart redis-server
sudo systemctl enable redis-server

# 8. é…ç½®å¤‡ä»½ç›®å½•
sudo mkdir -p /var/backups/knzn
sudo chown postgres:postgres /var/backups/knzn

# 9. å®‰è£…å¤‡ä»½è„šæœ¬
sudo tee /usr/local/bin/knzn-backup.sh << 'EOF'
#!/bin/bash
# KNZN æ•°æ®åº“å¤‡ä»½è„šæœ¬

BACKUP_DIR="/var/backups/knzn"
DATE=$(date +%Y%m%d_%H%M%S)
DB_NAME="knzn_production"
BACKUP_FILE="knzn_backup_${DATE}.sql"

# åˆ›å»ºå¤‡ä»½
pg_dump -h localhost -U knzn_user -d $DB_NAME > $BACKUP_DIR/$BACKUP_FILE

# å‹ç¼©å¤‡ä»½
gzip $BACKUP_DIR/$BACKUP_FILE

# ä¸Šä¼ åˆ° R2 (éœ€è¦é…ç½® AWS CLI)
aws s3 cp $BACKUP_DIR/${BACKUP_FILE}.gz \
    s3://knzn-backups/database/ \
    --endpoint-url https://$CLOUDFLARE_ACCOUNT_ID.r2.cloudflarestorage.com

# æ¸…ç†æœ¬åœ°å¤‡ä»½ (ä¿ç•™ 7 å¤©)
find $BACKUP_DIR -name "*.gz" -mtime +7 -delete

echo "Backup completed: ${BACKUP_FILE}.gz"
EOF

sudo chmod +x /usr/local/bin/knzn-backup.sh

# 10. é…ç½®å®šæ—¶å¤‡ä»½
sudo crontab -l | { cat; echo "0 2 * * * /usr/local/bin/knzn-backup.sh"; } | sudo crontab -

echo "âœ… VPS setup completed!"
echo "ğŸ—„ï¸ PostgreSQL: Ready"
echo "ğŸ”„ Redis: Ready"
echo "ğŸ”’ Firewall: Configured"
echo "ğŸ’¾ Backup: Scheduled"
```

### PostgreSQL ä¼˜åŒ–é…ç½®

```sql
-- postgresql-optimization.sql
-- KNZN PostgreSQL æ€§èƒ½ä¼˜åŒ–

-- 1. åˆ›å»ºå¿…è¦çš„ç´¢å¼•
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_email ON users(email);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_created_at ON users(created_at);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_progress_user_id ON progress(user_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_progress_lesson_id ON progress(lesson_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_progress_status ON progress(status);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_certificates_user_id ON certificates(user_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_community_posts_user_id ON community_posts(user_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_community_posts_status ON community_posts(status);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_community_posts_created_at ON community_posts(created_at);

-- 2. åˆ›å»ºå¤åˆç´¢å¼•
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_progress_user_status ON progress(user_id, status);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_posts_status_created ON community_posts(status, created_at DESC);

-- 3. åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯
ANALYZE users;
ANALYZE progress;
ANALYZE certificates;
ANALYZE community_posts;
ANALYZE blueprints;

-- 4. åˆ›å»ºè§†å›¾ä¼˜åŒ–å¸¸ç”¨æŸ¥è¯¢
CREATE OR REPLACE VIEW user_progress_summary AS
SELECT 
    u.id,
    u.name,
    u.level,
    u.xp,
    COUNT(p.id) as total_lessons,
    COUNT(CASE WHEN p.status = 'completed' THEN 1 END) as completed_lessons,
    COUNT(c.id) as certificates_earned
FROM users u
LEFT JOIN progress p ON u.id = p.user_id
LEFT JOIN certificates c ON u.id = c.user_id
GROUP BY u.id, u.name, u.level, u.xp;

-- 5. åˆ›å»ºæ’è¡Œæ¦œè§†å›¾
CREATE OR REPLACE VIEW leaderboard AS
SELECT 
    u.id,
    u.name,
    u.avatar_url,
    u.xp,
    u.level,
    COUNT(c.id) as certificates,
    ROW_NUMBER() OVER (ORDER BY u.xp DESC) as rank
FROM users u
LEFT JOIN certificates c ON u.id = c.user_id
WHERE u.xp > 0
GROUP BY u.id, u.name, u.avatar_url, u.xp, u.level
ORDER BY u.xp DESC
LIMIT 100;
```

## ğŸ“ Cloudflare R2 å­˜å‚¨é…ç½®

### R2 å­˜å‚¨æ¡¶è®¾ç½®

```bash
#!/bin/bash
# r2-setup.sh - Cloudflare R2 é…ç½®è„šæœ¬

# 1. å®‰è£… AWS CLI (ç”¨äº R2 æ“ä½œ)
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install

# 2. é…ç½® R2 å‡­æ®
aws configure set aws_access_key_id $R2_ACCESS_KEY_ID
aws configure set aws_secret_access_key $R2_SECRET_ACCESS_KEY
aws configure set region auto

# 3. åˆ›å»ºå­˜å‚¨æ¡¶
aws s3 mb s3://knzn-assets \
    --endpoint-url https://$CLOUDFLARE_ACCOUNT_ID.r2.cloudflarestorage.com

aws s3 mb s3://knzn-backups \
    --endpoint-url https://$CLOUDFLARE_ACCOUNT_ID.r2.cloudflarestorage.com

# 4. é…ç½® CORS
aws s3api put-bucket-cors \
    --bucket knzn-assets \
    --cors-configuration file://r2-cors.json \
    --endpoint-url https://$CLOUDFLARE_ACCOUNT_ID.r2.cloudflarestorage.com

echo "âœ… R2 storage configured!"
```

```json
// r2-cors.json
{
  "CORSRules": [
    {
      "AllowedHeaders": ["*"],
      "AllowedMethods": ["GET", "PUT", "POST", "DELETE"],
      "AllowedOrigins": [
        "https://knzn.net",
        "https://*.knzn.net",
        "http://localhost:3000"
      ],
      "ExposeHeaders": ["ETag"],
      "MaxAgeSeconds": 3600
    }
  ]
}
```

### æ–‡ä»¶ç®¡ç† API

```typescript
// server/api/storage/manage.post.ts
import { S3Client, ListObjectsV2Command, DeleteObjectCommand } from '@aws-sdk/client-s3'

const r2Client = new S3Client({
  region: 'auto',
  endpoint: `https://${process.env.CLOUDFLARE_ACCOUNT_ID}.r2.cloudflarestorage.com`,
  credentials: {
    accessKeyId: process.env.R2_ACCESS_KEY_ID!,
    secretAccessKey: process.env.R2_SECRET_ACCESS_KEY!,
  },
})

export default defineEventHandler(async (event) => {
  const body = await readBody(event)
  const { action, path, bucket = 'knzn-assets' } = body
  
  // ğŸ” éªŒè¯ç®¡ç†å‘˜æƒé™
  const session = await getUserSession(event)
  if (!session?.user?.adminRole) {
    throw createError({
      statusCode: 403,
      statusMessage: 'Admin access required'
    })
  }
  
  try {
    switch (action) {
      case 'list':
        return await listFiles(bucket, path)
      case 'delete':
        return await deleteFile(bucket, path)
      case 'cleanup':
        return await cleanupOrphanFiles(bucket)
      default:
        throw createError({
          statusCode: 400,
          statusMessage: 'Invalid action'
        })
    }
  } catch (error) {
    throw createError({
      statusCode: 500,
      statusMessage: 'Storage operation failed'
    })
  }
})

// ğŸ“‹ åˆ—å‡ºæ–‡ä»¶
const listFiles = async (bucket: string, prefix?: string) => {
  const command = new ListObjectsV2Command({
    Bucket: bucket,
    Prefix: prefix,
    MaxKeys: 1000
  })
  
  const response = await r2Client.send(command)
  
  return {
    files: response.Contents?.map(obj => ({
      key: obj.Key,
      size: obj.Size,
      lastModified: obj.LastModified,
      url: `https://assets.knzn.net/${obj.Key}`
    })) || [],
    totalSize: response.Contents?.reduce((sum, obj) => sum + (obj.Size || 0), 0) || 0
  }
}

// ğŸ—‘ï¸ åˆ é™¤æ–‡ä»¶
const deleteFile = async (bucket: string, key: string) => {
  const command = new DeleteObjectCommand({
    Bucket: bucket,
    Key: key
  })
  
  await r2Client.send(command)
  
  return { success: true, deletedKey: key }
}

// ğŸ§¹ æ¸…ç†å­¤å„¿æ–‡ä»¶
const cleanupOrphanFiles = async (bucket: string) => {
  // 1. è·å–æ‰€æœ‰æ–‡ä»¶
  const { files } = await listFiles(bucket, 'uploads/')
  
  // 2. æŸ¥è¯¢æ•°æ®åº“ä¸­å¼•ç”¨çš„æ–‡ä»¶
  const referencedFiles = await db.select({ url: blueprints.coverImage })
    .from(blueprints)
    .where(isNotNull(blueprints.coverImage))
  
  const referencedKeys = new Set(
    referencedFiles
      .map(f => f.url?.replace('https://assets.knzn.net/', ''))
      .filter(Boolean)
  )
  
  // 3. æ‰¾å‡ºå­¤å„¿æ–‡ä»¶ (è¶…è¿‡ 7 å¤©ä¸”æœªè¢«å¼•ç”¨)
  const orphanFiles = files.filter(file => {
    const isOld = file.lastModified && 
      (Date.now() - new Date(file.lastModified).getTime()) > 7 * 24 * 60 * 60 * 1000
    const isOrphan = !referencedKeys.has(file.key!)
    return isOld && isOrphan
  })
  
  // 4. åˆ é™¤å­¤å„¿æ–‡ä»¶
  const deletedFiles = []
  for (const file of orphanFiles) {
    if (file.key) {
      await deleteFile(bucket, file.key)
      deletedFiles.push(file.key)
    }
  }
  
  return {
    deletedCount: deletedFiles.length,
    deletedFiles,
    freedSpace: orphanFiles.reduce((sum, f) => sum + (f.size || 0), 0)
  }
}
```
## ğŸ“§ Resend é‚®ä»¶æœåŠ¡é…ç½®

### DNS é…ç½®

```bash
# DNS è®°å½•é…ç½® (åœ¨åŸŸåæä¾›å•†å¤„è®¾ç½®)

# SPF è®°å½• (TXT)
knzn.net. IN TXT "v=spf1 include:_spf.resend.com ~all"

# DKIM è®°å½• (CNAME)
resend._domainkey.knzn.net. IN CNAME resend._domainkey.resend.com.

# DMARC è®°å½• (TXT)
_dmarc.knzn.net. IN TXT "v=DMARC1; p=quarantine; rua=mailto:dmarc@knzn.net; ruf=mailto:dmarc@knzn.net; fo=1"

# MX è®°å½• (ç”¨äºæ¥æ”¶é‚®ä»¶ï¼Œå¯é€‰)
knzn.net. IN MX 10 mx1.resend.com.
knzn.net. IN MX 20 mx2.resend.com.
```

### é‚®ä»¶ç›‘æ§ç³»ç»Ÿ

```typescript
// server/api/admin/email/stats.get.ts
export default defineEventHandler(async (event) => {
  // ğŸ” éªŒè¯ç®¡ç†å‘˜æƒé™
  const admin = await getAdminUser(event)
  if (!admin) {
    throw createError({ statusCode: 403, statusMessage: 'Admin access required' })
  }
  
  try {
    // ğŸ“Š è·å– Resend ç»Ÿè®¡æ•°æ®
    const response = await fetch('https://api.resend.com/emails', {
      headers: {
        'Authorization': `Bearer ${process.env.RESEND_API_KEY}`,
        'Content-Type': 'application/json'
      }
    })
    
    if (!response.ok) {
      throw new Error('Failed to fetch email stats')
    }
    
    const emailData = await response.json()
    
    // ğŸ“ˆ è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    const stats = {
      totalSent: emailData.data?.length || 0,
      deliveryRate: calculateDeliveryRate(emailData.data),
      bounceRate: calculateBounceRate(emailData.data),
      openRate: calculateOpenRate(emailData.data),
      recentEmails: emailData.data?.slice(0, 10) || []
    }
    
    return stats
  } catch (error) {
    throw createError({
      statusCode: 500,
      statusMessage: 'Failed to fetch email statistics'
    })
  }
})

const calculateDeliveryRate = (emails: any[]) => {
  if (!emails.length) return 0
  const delivered = emails.filter(email => email.last_event === 'delivered').length
  return (delivered / emails.length * 100).toFixed(2)
}

const calculateBounceRate = (emails: any[]) => {
  if (!emails.length) return 0
  const bounced = emails.filter(email => email.last_event === 'bounced').length
  return (bounced / emails.length * 100).toFixed(2)
}

const calculateOpenRate = (emails: any[]) => {
  if (!emails.length) return 0
  const opened = emails.filter(email => email.last_event === 'opened').length
  return (opened / emails.length * 100).toFixed(2)
}
```

## ğŸ”„ è‡ªåŠ¨åŒ–å¤‡ä»½ç³»ç»Ÿ

### å¢å¼ºå¤‡ä»½è„šæœ¬

```bash
#!/bin/bash
# /usr/local/bin/knzn-backup-enhanced.sh
# KNZN å¢å¼ºå¤‡ä»½è„šæœ¬

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# ğŸ“‹ é…ç½®å˜é‡
BACKUP_DIR="/var/backups/knzn"
LOG_FILE="/var/log/knzn-backup.log"
DATE=$(date +%Y%m%d_%H%M%S)
DB_NAME="knzn_production"
BACKUP_FILE="knzn_backup_${DATE}.sql"
ENCRYPTED_FILE="${BACKUP_FILE}.gz.enc"
RETENTION_DAYS=30

# ğŸ“ æ—¥å¿—å‡½æ•°
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a $LOG_FILE
}

# ğŸš¨ é”™è¯¯å¤„ç†
error_exit() {
    log "ERROR: $1"
    # å‘é€å‘Šè­¦é‚®ä»¶
    curl -X POST "https://knzn.net/api/admin/alerts/backup-failed" \
        -H "Content-Type: application/json" \
        -H "X-API-Key: $INTERNAL_API_KEY" \
        -d "{\"error\": \"$1\", \"timestamp\": \"$(date -Iseconds)\"}"
    exit 1
}

log "Starting KNZN database backup..."

# ğŸ—‚ï¸ åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR || error_exit "Failed to create backup directory"

# ğŸ—„ï¸ æ•°æ®åº“å¤‡ä»½
log "Creating database dump..."
pg_dump -h localhost -U knzn_user -d $DB_NAME > $BACKUP_DIR/$BACKUP_FILE || error_exit "Database dump failed"

# ğŸ“ æ£€æŸ¥å¤‡ä»½æ–‡ä»¶å¤§å°
BACKUP_SIZE=$(stat -c%s "$BACKUP_DIR/$BACKUP_FILE")
if [ $BACKUP_SIZE -lt 1000 ]; then
    error_exit "Backup file too small: ${BACKUP_SIZE} bytes"
fi

log "Database dump completed: ${BACKUP_SIZE} bytes"

# ğŸ—œï¸ å‹ç¼©å¤‡ä»½
log "Compressing backup..."
gzip $BACKUP_DIR/$BACKUP_FILE || error_exit "Compression failed"

# ğŸ” åŠ å¯†å¤‡ä»½
log "Encrypting backup..."
openssl enc -aes-256-cbc -salt \
    -in $BACKUP_DIR/${BACKUP_FILE}.gz \
    -out $BACKUP_DIR/$ENCRYPTED_FILE \
    -k $BACKUP_ENCRYPTION_KEY || error_exit "Encryption failed"

# ğŸ“¤ ä¸Šä¼ åˆ° R2
log "Uploading to Cloudflare R2..."
aws s3 cp $BACKUP_DIR/$ENCRYPTED_FILE \
    s3://knzn-backups/database/$ENCRYPTED_FILE \
    --endpoint-url https://$CLOUDFLARE_ACCOUNT_ID.r2.cloudflarestorage.com || error_exit "Upload to R2 failed"

# âœ… éªŒè¯ä¸Šä¼ 
aws s3 ls s3://knzn-backups/database/$ENCRYPTED_FILE \
    --endpoint-url https://$CLOUDFLARE_ACCOUNT_ID.r2.cloudflarestorage.com > /dev/null || error_exit "Upload verification failed"

log "Backup uploaded successfully: $ENCRYPTED_FILE"

# ğŸ§¹ æ¸…ç†æœ¬åœ°æ–‡ä»¶
rm -f $BACKUP_DIR/${BACKUP_FILE}.gz
rm -f $BACKUP_DIR/$ENCRYPTED_FILE

# ğŸ—‘ï¸ æ¸…ç†æ—§å¤‡ä»½ (ä¿ç•™30å¤©)
log "Cleaning up old backups..."
CUTOFF_DATE=$(date -d "$RETENTION_DAYS days ago" +%Y%m%d)

aws s3 ls s3://knzn-backups/database/ \
    --endpoint-url https://$CLOUDFLARE_ACCOUNT_ID.r2.cloudflarestorage.com \
    | awk -v cutoff="$CUTOFF_DATE" '$1 < cutoff {print $4}' \
    | while read -r old_backup; do
        if [ -n "$old_backup" ]; then
            aws s3 rm s3://knzn-backups/database/$old_backup \
                --endpoint-url https://$CLOUDFLARE_ACCOUNT_ID.r2.cloudflarestorage.com
            log "Deleted old backup: $old_backup"
        fi
    done

# ğŸ“Š è®°å½•å¤‡ä»½ç»Ÿè®¡
FINAL_SIZE=$(aws s3api head-object \
    --bucket knzn-backups \
    --key database/$ENCRYPTED_FILE \
    --endpoint-url https://$CLOUDFLARE_ACCOUNT_ID.r2.cloudflarestorage.com \
    --query ContentLength --output text)

log "Backup completed successfully!"
log "Final encrypted size: $FINAL_SIZE bytes"
log "Compression ratio: $(echo "scale=2; $FINAL_SIZE * 100 / $BACKUP_SIZE" | bc)%"

# ğŸ“ˆ å‘é€æˆåŠŸé€šçŸ¥
curl -X POST "https://knzn.net/api/admin/alerts/backup-success" \
    -H "Content-Type: application/json" \
    -H "X-API-Key: $INTERNAL_API_KEY" \
    -d "{
        \"backup_file\": \"$ENCRYPTED_FILE\",
        \"original_size\": $BACKUP_SIZE,
        \"compressed_size\": $FINAL_SIZE,
        \"timestamp\": \"$(date -Iseconds)\"
    }"

log "Backup process completed successfully!"
```

### å¤‡ä»½æ¢å¤è„šæœ¬

```bash
#!/bin/bash
# /usr/local/bin/knzn-restore.sh
# KNZN æ•°æ®åº“æ¢å¤è„šæœ¬

set -e

# ğŸ“‹ é…ç½®
RESTORE_DIR="/tmp/knzn-restore"
LOG_FILE="/var/log/knzn-restore.log"
DB_NAME="knzn_production"

# ğŸ“ æ—¥å¿—å‡½æ•°
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a $LOG_FILE
}

# ğŸš¨ é”™è¯¯å¤„ç†
error_exit() {
    log "ERROR: $1"
    exit 1
}

# ğŸ“¥ å‚æ•°æ£€æŸ¥
if [ $# -ne 1 ]; then
    echo "Usage: $0 <backup_filename>"
    echo "Example: $0 knzn_backup_20241223_020000.sql.gz.enc"
    exit 1
fi

BACKUP_FILE=$1

log "Starting database restore from: $BACKUP_FILE"

# âš ï¸ å®‰å…¨ç¡®è®¤
echo "âš ï¸  WARNING: This will REPLACE the current database!"
echo "Database: $DB_NAME"
echo "Backup file: $BACKUP_FILE"
echo ""
read -p "Are you sure you want to continue? (type 'YES' to confirm): " CONFIRM

if [ "$CONFIRM" != "YES" ]; then
    log "Restore cancelled by user"
    exit 0
fi

# ğŸ—‚ï¸ åˆ›å»ºæ¢å¤ç›®å½•
mkdir -p $RESTORE_DIR || error_exit "Failed to create restore directory"

# ğŸ“¥ ä¸‹è½½å¤‡ä»½æ–‡ä»¶
log "Downloading backup from R2..."
aws s3 cp s3://knzn-backups/database/$BACKUP_FILE \
    $RESTORE_DIR/$BACKUP_FILE \
    --endpoint-url https://$CLOUDFLARE_ACCOUNT_ID.r2.cloudflarestorage.com || error_exit "Download failed"

# ğŸ”“ è§£å¯†å¤‡ä»½
log "Decrypting backup..."
openssl enc -aes-256-cbc -d \
    -in $RESTORE_DIR/$BACKUP_FILE \
    -out $RESTORE_DIR/restore.sql.gz \
    -k $BACKUP_ENCRYPTION_KEY || error_exit "Decryption failed"

# ğŸ—œï¸ è§£å‹å¤‡ä»½
log "Decompressing backup..."
gunzip $RESTORE_DIR/restore.sql.gz || error_exit "Decompression failed"

# ğŸ’¾ åˆ›å»ºå½“å‰æ•°æ®åº“å¤‡ä»½ (å®‰å…¨æªæ–½)
log "Creating safety backup of current database..."
SAFETY_BACKUP="safety_backup_$(date +%Y%m%d_%H%M%S).sql"
pg_dump -h localhost -U knzn_user -d $DB_NAME > $RESTORE_DIR/$SAFETY_BACKUP || error_exit "Safety backup failed"

log "Safety backup created: $SAFETY_BACKUP"

# ğŸ”„ åœæ­¢åº”ç”¨è¿æ¥ (å¯é€‰)
log "Terminating active connections..."
sudo -u postgres psql -c "
    SELECT pg_terminate_backend(pid) 
    FROM pg_stat_activity 
    WHERE datname = '$DB_NAME' AND pid <> pg_backend_pid();
"

# ğŸ—„ï¸ æ¢å¤æ•°æ®åº“
log "Restoring database..."
sudo -u postgres psql -c "DROP DATABASE IF EXISTS ${DB_NAME}_old;"
sudo -u postgres psql -c "ALTER DATABASE $DB_NAME RENAME TO ${DB_NAME}_old;"
sudo -u postgres psql -c "CREATE DATABASE $DB_NAME OWNER knzn_user;"

psql -h localhost -U knzn_user -d $DB_NAME < $RESTORE_DIR/restore.sql || {
    log "Restore failed! Rolling back..."
    sudo -u postgres psql -c "DROP DATABASE $DB_NAME;"
    sudo -u postgres psql -c "ALTER DATABASE ${DB_NAME}_old RENAME TO $DB_NAME;"
    error_exit "Database restore failed and rolled back"
}

# âœ… éªŒè¯æ¢å¤
log "Verifying restore..."
TABLE_COUNT=$(psql -h localhost -U knzn_user -d $DB_NAME -t -c "
    SELECT COUNT(*) FROM information_schema.tables 
    WHERE table_schema = 'public';
")

if [ "$TABLE_COUNT" -lt 5 ]; then
    error_exit "Restore verification failed: insufficient tables ($TABLE_COUNT)"
fi

log "Restore verification passed: $TABLE_COUNT tables found"

# ğŸ§¹ æ¸…ç†
log "Cleaning up..."
rm -rf $RESTORE_DIR

# ğŸ—‘ï¸ åˆ é™¤æ—§æ•°æ®åº“
sudo -u postgres psql -c "DROP DATABASE ${DB_NAME}_old;"

log "Database restore completed successfully!"
log "Restored from: $BACKUP_FILE"
log "Tables restored: $TABLE_COUNT"

# ğŸ“ˆ å‘é€æ¢å¤é€šçŸ¥
curl -X POST "https://knzn.net/api/admin/alerts/restore-success" \
    -H "Content-Type: application/json" \
    -H "X-API-Key: $INTERNAL_API_KEY" \
    -d "{
        \"backup_file\": \"$BACKUP_FILE\",
        \"tables_restored\": $TABLE_COUNT,
        \"timestamp\": \"$(date -Iseconds)\"
    }"

echo "âœ… Database restore completed successfully!"
```

## ğŸ“Š ç›‘æ§ä¸å‘Šè­¦ç³»ç»Ÿ

### ç³»ç»Ÿç›‘æ§è„šæœ¬

```bash
#!/bin/bash
# /usr/local/bin/knzn-monitor.sh
# KNZN ç³»ç»Ÿç›‘æ§è„šæœ¬

# ğŸ“‹ é…ç½®
ALERT_ENDPOINT="https://knzn.net/api/admin/alerts/system"
LOG_FILE="/var/log/knzn-monitor.log"

# ğŸ“ æ—¥å¿—å‡½æ•°
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a $LOG_FILE
}

# ğŸš¨ å‘é€å‘Šè­¦
send_alert() {
    local severity=$1
    local message=$2
    local metric=$3
    local value=$4
    
    curl -X POST $ALERT_ENDPOINT \
        -H "Content-Type: application/json" \
        -H "X-API-Key: $INTERNAL_API_KEY" \
        -d "{
            \"severity\": \"$severity\",
            \"message\": \"$message\",
            \"metric\": \"$metric\",
            \"value\": \"$value\",
            \"timestamp\": \"$(date -Iseconds)\",
            \"hostname\": \"$(hostname)\"
        }" || log "Failed to send alert: $message"
}

log "Starting system monitoring check..."

# ğŸ’¾ æ£€æŸ¥ç£ç›˜ä½¿ç”¨ç‡
DISK_USAGE=$(df / | awk 'NR==2 {print $5}' | sed 's/%//')
if [ $DISK_USAGE -gt 85 ]; then
    send_alert "critical" "Disk usage critical: ${DISK_USAGE}%" "disk_usage" "$DISK_USAGE"
elif [ $DISK_USAGE -gt 75 ]; then
    send_alert "warning" "Disk usage high: ${DISK_USAGE}%" "disk_usage" "$DISK_USAGE"
fi

# ğŸ§  æ£€æŸ¥å†…å­˜ä½¿ç”¨ç‡
MEMORY_USAGE=$(free | awk 'NR==2{printf "%.0f", $3*100/$2}')
if [ $MEMORY_USAGE -gt 90 ]; then
    send_alert "critical" "Memory usage critical: ${MEMORY_USAGE}%" "memory_usage" "$MEMORY_USAGE"
elif [ $MEMORY_USAGE -gt 80 ]; then
    send_alert "warning" "Memory usage high: ${MEMORY_USAGE}%" "memory_usage" "$MEMORY_USAGE"
fi

# ğŸ”„ æ£€æŸ¥ PostgreSQL çŠ¶æ€
if ! systemctl is-active --quiet postgresql; then
    send_alert "critical" "PostgreSQL service is down" "postgresql_status" "down"
else
    # æ£€æŸ¥æ•°æ®åº“è¿æ¥
    if ! sudo -u postgres psql -d knzn_production -c "SELECT 1;" > /dev/null 2>&1; then
        send_alert "critical" "PostgreSQL connection failed" "postgresql_connection" "failed"
    fi
    
    # æ£€æŸ¥æ•°æ®åº“å¤§å°
    DB_SIZE=$(sudo -u postgres psql -d knzn_production -t -c "
        SELECT pg_size_pretty(pg_database_size('knzn_production'));
    " | xargs)
    log "Database size: $DB_SIZE"
fi

# ğŸ”„ æ£€æŸ¥ Redis çŠ¶æ€
if ! systemctl is-active --quiet redis-server; then
    send_alert "critical" "Redis service is down" "redis_status" "down"
else
    # æ£€æŸ¥ Redis è¿æ¥
    if ! redis-cli ping > /dev/null 2>&1; then
        send_alert "critical" "Redis connection failed" "redis_connection" "failed"
    fi
fi

# ğŸŒ æ£€æŸ¥ç½‘ç»œè¿æ¥
if ! curl -f -s https://knzn.net/api/health > /dev/null; then
    send_alert "critical" "Website health check failed" "website_health" "failed"
fi

# ğŸ”’ æ£€æŸ¥ SSL è¯ä¹¦ (å¦‚æœä½¿ç”¨è‡ªç­¾å)
if command -v openssl > /dev/null; then
    SSL_EXPIRY=$(echo | openssl s_client -servername knzn.net -connect knzn.net:443 2>/dev/null | openssl x509 -noout -dates | grep notAfter | cut -d= -f2)
    SSL_EXPIRY_EPOCH=$(date -d "$SSL_EXPIRY" +%s)
    CURRENT_EPOCH=$(date +%s)
    DAYS_UNTIL_EXPIRY=$(( (SSL_EXPIRY_EPOCH - CURRENT_EPOCH) / 86400 ))
    
    if [ $DAYS_UNTIL_EXPIRY -lt 7 ]; then
        send_alert "critical" "SSL certificate expires in $DAYS_UNTIL_EXPIRY days" "ssl_expiry" "$DAYS_UNTIL_EXPIRY"
    elif [ $DAYS_UNTIL_EXPIRY -lt 30 ]; then
        send_alert "warning" "SSL certificate expires in $DAYS_UNTIL_EXPIRY days" "ssl_expiry" "$DAYS_UNTIL_EXPIRY"
    fi
fi

log "System monitoring check completed"
```

### å‘Šè­¦å¤„ç† API

```typescript
// server/api/admin/alerts/[type].post.ts
export default defineEventHandler(async (event) => {
  const alertType = getRouterParam(event, 'type')
  const body = await readBody(event)
  
  // ğŸ” éªŒè¯ API å¯†é’¥
  const apiKey = getHeader(event, 'x-api-key')
  if (apiKey !== process.env.INTERNAL_API_KEY) {
    throw createError({
      statusCode: 401,
      statusMessage: 'Invalid API key'
    })
  }
  
  try {
    // ğŸ“ è®°å½•å‘Šè­¦åˆ°æ•°æ®åº“
    await db.insert(systemAlerts).values({
      id: nanoid(),
      type: alertType,
      severity: body.severity || 'info',
      message: body.message,
      metadata: body,
      createdAt: new Date()
    })
    
    // ğŸš¨ å¤„ç†ä¸åŒç±»å‹çš„å‘Šè­¦
    switch (alertType) {
      case 'backup-failed':
        await handleBackupFailure(body)
        break
      case 'backup-success':
        await handleBackupSuccess(body)
        break
      case 'system':
        await handleSystemAlert(body)
        break
      case 'restore-success':
        await handleRestoreSuccess(body)
        break
    }
    
    return { success: true, alertId: nanoid() }
  } catch (error) {
    console.error('Alert processing error:', error)
    throw createError({
      statusCode: 500,
      statusMessage: 'Alert processing failed'
    })
  }
})

// ğŸ’¾ å¤‡ä»½å¤±è´¥å¤„ç†
const handleBackupFailure = async (alert: any) => {
  // ğŸ“§ å‘é€ç´§æ€¥é‚®ä»¶é€šçŸ¥
  await sendEmail({
    to: 'admin@knzn.net',
    template: 'backup-failure-alert',
    data: {
      error: alert.error,
      timestamp: alert.timestamp,
      hostname: alert.hostname || 'unknown'
    }
  })
  
  // ğŸ“± å‘é€ Slack é€šçŸ¥ (å¦‚æœé…ç½®)
  if (process.env.SLACK_WEBHOOK_URL) {
    await fetch(process.env.SLACK_WEBHOOK_URL, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        text: `ğŸš¨ KNZN Backup Failed: ${alert.error}`,
        channel: '#alerts',
        username: 'KNZN Monitor'
      })
    })
  }
}

// âœ… å¤‡ä»½æˆåŠŸå¤„ç†
const handleBackupSuccess = async (alert: any) => {
  // ğŸ“Š æ›´æ–°å¤‡ä»½ç»Ÿè®¡
  await db.insert(backupStats).values({
    id: nanoid(),
    backupFile: alert.backup_file,
    originalSize: alert.original_size,
    compressedSize: alert.compressed_size,
    compressionRatio: (alert.compressed_size / alert.original_size * 100).toFixed(2),
    createdAt: new Date()
  })
}

// ğŸ–¥ï¸ ç³»ç»Ÿå‘Šè­¦å¤„ç†
const handleSystemAlert = async (alert: any) => {
  const { severity, message, metric, value } = alert
  
  // ğŸš¨ ä¸¥é‡å‘Šè­¦ç«‹å³é€šçŸ¥
  if (severity === 'critical') {
    await sendEmail({
      to: 'admin@knzn.net',
      template: 'system-critical-alert',
      data: {
        message,
        metric,
        value,
        timestamp: alert.timestamp,
        hostname: alert.hostname
      }
    })
  }
  
  // ğŸ“Š è®°å½•æŒ‡æ ‡å†å²
  await db.insert(systemMetrics).values({
    id: nanoid(),
    metric,
    value: parseFloat(value) || 0,
    severity,
    createdAt: new Date()
  })
}
```

## ğŸ’° æˆæœ¬ä¼˜åŒ–ç­–ç•¥

### æœˆåº¦æˆæœ¬åˆ†æ

```typescript
// æˆæœ¬é¢„ç®—åˆ†æ
const MONTHLY_COSTS = {
  // ğŸ–¥ï¸ åŸºç¡€è®¾æ–½æˆæœ¬
  infrastructure: {
    contabo: {
      plan: 'VPS L',
      specs: '12GB RAM, 6 CPU cores, 100GB NVMe',
      cost: 13,
      description: 'å®Œå…¨ç§æœ‰åŒ–éƒ¨ç½²'
    },
      description: 'æ— é™å¸¦å®½ï¼Œè¾¹ç¼˜å‡½æ•°ï¼Œåˆ†æ'
    },
    vps: {
      provider: 'DigitalOcean Droplet',
      specs: '2GB RAM, 1 vCPU, 50GB SSD',
      cost: 12,
      description: 'PostgreSQL + Redis æœåŠ¡å™¨'
    },
    cloudflareR2: {
      storage: '50GB',
      requests: '1M/æœˆ',
      cost: 8,
      description: 'æ–‡ä»¶å­˜å‚¨ + CDN'
    }
  },
  
  // ğŸ› ï¸ ç¬¬ä¸‰æ–¹æœåŠ¡æˆæœ¬
  services: {
    resend: {
      plan: 'Pro',
      emails: '100K/æœˆ',
      cost: 20,
      description: 'é‚®ä»¶å‘é€æœåŠ¡'
    },
    openai: {
      model: 'gpt-4o-mini',
      tokens: '10M tokens/æœˆ',
      cost: 15,
      description: 'AI åŠ©æ•™æœåŠ¡'
    },
    lemonSqueezy: {
      commission: '5% + $0.50 per transaction',
      cost: 0, // ä»æ”¶å…¥ä¸­æ‰£é™¤
      description: 'æ”¯ä»˜å¤„ç†è´¹ç”¨'
    }
  },
  
  // ğŸ“Š æ€»æˆæœ¬
  totalMonthlyCost: 75, // $75/æœˆ
  
  // ğŸ¯ æ”¶å…¥ç›®æ ‡
  revenueTarget: {
    proUsers: 1000,
    pricePerUser: 9.99,
    monthlyRevenue: 9990,
    netProfit: 9915, // 99.2% åˆ©æ¶¦ç‡
    profitMargin: '99.2%'
  },
  
  // ğŸ“ˆ æ‰©å±•æˆæœ¬ (10K ç”¨æˆ·)
  scalingCosts: {
    contabo: 26, // å‡çº§åˆ° VPS XL (16GB RAM, 8 CPU cores)
    vps: 0, // ä¸å†éœ€è¦å•ç‹¬çš„æ•°æ®åº“ VPS
    r2: 25, // 200GB å­˜å‚¨
    resend: 40, // 500K é‚®ä»¶
    openai: 50, // 50M tokens
    total: 159 // $159/æœˆ
  }
}
```

### æˆæœ¬ç›‘æ§ API

```typescript
// server/api/admin/costs/analysis.get.ts
export default defineEventHandler(async (event) => {
  const admin = await getAdminUser(event)
  if (!admin) {
    throw createError({ statusCode: 403, statusMessage: 'Admin access required' })
  }
  
  const currentMonth = new Date().toISOString().slice(0, 7) // YYYY-MM
  
  try {
    // ğŸ“Š è®¡ç®—å„é¡¹æˆæœ¬
    const costs = {
      // ğŸ—„ï¸ æ•°æ®åº“å­˜å‚¨æˆæœ¬
      database: await calculateDatabaseCosts(),
      
      // ğŸ“ æ–‡ä»¶å­˜å‚¨æˆæœ¬
      storage: await calculateStorageCosts(),
      
      // ğŸ“§ é‚®ä»¶å‘é€æˆæœ¬
      email: await calculateEmailCosts(currentMonth),
      
      // ğŸ¤– AI ä½¿ç”¨æˆæœ¬
      ai: await calculateAICosts(currentMonth),
      
      // ğŸ’³ æ”¯ä»˜å¤„ç†æˆæœ¬
      payment: await calculatePaymentCosts(currentMonth)
    }
    
    // ğŸ“ˆ è®¡ç®—æ€»æˆæœ¬å’Œåˆ©æ¶¦ç‡
    const totalCosts = Object.values(costs).reduce((sum, cost) => sum + cost.amount, 0)
    const revenue = await calculateMonthlyRevenue(currentMonth)
    const profitMargin = revenue > 0 ? ((revenue - totalCosts) / revenue * 100).toFixed(2) : 0
    
    return {
      costs,
      totalCosts,
      revenue,
      profit: revenue - totalCosts,
      profitMargin: `${profitMargin}%`,
      month: currentMonth
    }
  } catch (error) {
    throw createError({
      statusCode: 500,
      statusMessage: 'Cost analysis failed'
    })
  }
})

// ğŸ’¾ æ•°æ®åº“æˆæœ¬è®¡ç®—
const calculateDatabaseCosts = async () => {
  const dbSize = await db.execute(sql`
    SELECT pg_size_pretty(pg_database_size('knzn_production')) as size,
           pg_database_size('knzn_production') as bytes
  `)
  
  const sizeGB = dbSize[0].bytes / (1024 * 1024 * 1024)
  const monthlyCost = sizeGB > 50 ? 12 + (sizeGB - 50) * 0.1 : 12 // åŸºç¡€ $12ï¼Œè¶…å‡º 50GB æ¯ GB $0.1
  
  return {
    amount: monthlyCost,
    details: {
      size: dbSize[0].size,
      sizeGB: sizeGB.toFixed(2),
      baseCost: 12,
      overageCost: Math.max(0, (sizeGB - 50) * 0.1)
    }
  }
}

// ğŸ“ å­˜å‚¨æˆæœ¬è®¡ç®—
const calculateStorageCosts = async () => {
  // é€šè¿‡ R2 API è·å–å­˜å‚¨ä½¿ç”¨é‡
  const storageUsage = await fetch(`https://api.cloudflare.com/client/v4/accounts/${process.env.CLOUDFLARE_ACCOUNT_ID}/r2/buckets/knzn-assets/usage`, {
    headers: {
      'Authorization': `Bearer ${process.env.CLOUDFLARE_API_TOKEN}`,
      'Content-Type': 'application/json'
    }
  }).then(res => res.json())
  
  const storageGB = storageUsage.result?.storage_bytes / (1024 * 1024 * 1024) || 0
  const requestCount = storageUsage.result?.requests || 0
  
  const storageCost = storageGB * 0.015 // $0.015 per GB
  const requestCost = requestCount * 0.0000004 // $0.0000004 per request
  
  return {
    amount: storageCost + requestCost,
    details: {
      storageGB: storageGB.toFixed(2),
      requestCount,
      storageCost: storageCost.toFixed(4),
      requestCost: requestCost.toFixed(4)
    }
  }
}
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v2.0 - KNZN ä¸“ç”¨ç‰ˆ  
**æœ€åæ›´æ–°**: 2024-12-23  
**é€‚ç”¨é¡¹ç›®**: KNZN ç¡¬ä»¶å­¦ä¹ å¹³å°  
**éƒ¨ç½²ç­–ç•¥**: Contabo VPS å•æœºå®¹å™¨åŒ–é›†ç¾¤

è¿™ä»½è¿ç»´æŒ‡å—ä¸“ä¸º Contabo VPS çš„å•æœºå®¹å™¨åŒ–éƒ¨ç½²è®¾è®¡ï¼Œé€šè¿‡ Docker Compose ç¼–æ’å®ç°å®Œæ•´çš„åº”ç”¨æ ˆï¼Œç›¸æ¯” Vercel æ··åˆæ–¹æ¡ˆè¿›ä¸€æ­¥é™ä½äº†æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†é«˜å¯ç”¨æ€§å’Œæ˜“ç»´æŠ¤æ€§ã€‚
### SSL é…ç½®æ–‡ä»¶

```nginx
# docker/nginx/ssl.conf
# ğŸ”’ SSL å®‰å…¨é…ç½®

ssl_protocols TLSv1.2 TLSv1.3;
ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA:ECDHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!SRP:!CAMELLIA;
ssl_prefer_server_ciphers off;

# ğŸ” SSL ä¼šè¯é…ç½®
ssl_session_cache shared:SSL:10m;
ssl_session_timeout 10m;
ssl_session_tickets off;

# ğŸ”’ HSTS (HTTP Strict Transport Security)
add_header Strict-Transport-Security "max-age=63072000; includeSubDomains; preload" always;

# ğŸ” OCSP Stapling
ssl_stapling on;
ssl_stapling_verify on;
ssl_trusted_certificate /etc/letsencrypt/live/knzn.net/chain.pem;
resolver 8.8.8.8 8.8.4.4 valid=300s;
resolver_timeout 5s;
```

### ç¼“å­˜é…ç½®æ–‡ä»¶

```nginx
# docker/nginx/cache.conf
# ğŸ“ é™æ€èµ„æºç¼“å­˜é…ç½® (æ›¿ä»£ Vercel Edge åŠŸèƒ½)

# ğŸ–¼ï¸ å›¾ç‰‡ç¼“å­˜ (1å¹´)
location ~* \.(jpg|jpeg|png|gif|ico|svg|webp|avif)$ {
    expires 1y;
    add_header Cache-Control "public, immutable";
    add_header Vary "Accept-Encoding";
    
    # ğŸ—œï¸ å‹ç¼©
    gzip_static on;
    
    # ğŸ“Š è®¿é—®æ—¥å¿—å…³é—­
    access_log off;
}

# ğŸ¨ CSS/JS ç¼“å­˜ (1å¹´ï¼Œå¸¦ç‰ˆæœ¬å·)
location ~* \.(css|js|mjs)$ {
    expires 1y;
    add_header Cache-Control "public, immutable";
    add_header Vary "Accept-Encoding";
    
    # ğŸ—œï¸ å‹ç¼©
    gzip_static on;
    
    # ğŸ“Š è®¿é—®æ—¥å¿—å…³é—­
    access_log off;
}

# ğŸ¬ åª’ä½“æ–‡ä»¶ç¼“å­˜ (1ä¸ªæœˆ)
location ~* \.(mp4|webm|ogg|mp3|wav|flac|aac)$ {
    expires 1M;
    add_header Cache-Control "public";
    add_header Vary "Accept-Encoding";
    
    # ğŸ“Š è®¿é—®æ—¥å¿—å…³é—­
    access_log off;
}

# ğŸ“„ å­—ä½“æ–‡ä»¶ç¼“å­˜ (1å¹´)
location ~* \.(woff|woff2|ttf|eot)$ {
    expires 1y;
    add_header Cache-Control "public, immutable";
    add_header Access-Control-Allow-Origin "*";
    
    # ğŸ“Š è®¿é—®æ—¥å¿—å…³é—­
    access_log off;
}

# ğŸ”„ API ç¼“å­˜ (çŸ­æœŸ)
location /api/ {
    # ğŸš« ç¦ç”¨ç¼“å­˜ (åŠ¨æ€å†…å®¹)
    add_header Cache-Control "no-cache, no-store, must-revalidate";
    add_header Pragma "no-cache";
    add_header Expires "0";
    
    # ğŸš€ ä»£ç†åˆ°åº”ç”¨
    proxy_pass http://nuxt_app;
    proxy_http_version 1.1;
    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection 'upgrade';
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
    proxy_cache_bypass $http_upgrade;
}

# ğŸ“„ HTML ç¼“å­˜ (çŸ­æœŸ)
location ~* \.html$ {
    expires 5m;
    add_header Cache-Control "public, must-revalidate";
    add_header Vary "Accept-Encoding";
}
```

## ğŸš€ CI/CD è‡ªåŠ¨åŒ–éƒ¨ç½²

### GitHub Actions å·¥ä½œæµ

```yaml
# .github/workflows/deploy.yml
name: Deploy to Contabo VPS

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: knzn-app

jobs:
  # ğŸ§ª æµ‹è¯•é˜¶æ®µ
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      
      - name: Install pnpm
        run: npm install -g pnpm
      
      - name: Install dependencies
        run: pnpm install --frozen-lockfile
      
      - name: Run type check
        run: pnpm run type-check
      
      - name: Run linting
        run: pnpm run lint
      
      - name: Run tests
        run: pnpm run test

  # ğŸ—ï¸ æ„å»ºé˜¶æ®µ
  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    permissions:
      contents: read
      packages: write
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ github.repository }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}
      
      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./docker/app/Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          target: production

  # ğŸš€ éƒ¨ç½²é˜¶æ®µ
  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Deploy to Contabo VPS
        uses: appleboy/ssh-action@v1.0.0
        with:
          host: ${{ secrets.VPS_HOST }}
          username: ${{ secrets.VPS_USER }}
          key: ${{ secrets.VPS_SSH_KEY }}
          script: |
            # ğŸ”„ æ›´æ–°ä»£ç 
            cd /opt/knzn
            git pull origin main
            
            # ğŸ³ ç™»å½•åˆ° GitHub Container Registry
            echo ${{ secrets.GITHUB_TOKEN }} | docker login ghcr.io -u ${{ github.actor }} --password-stdin
            
            # ğŸ“¥ æ‹‰å–æœ€æ–°é•œåƒ
            docker-compose -f docker-compose.yml -f docker-compose.prod.yml pull app
            
            # ğŸ”„ é‡å¯æœåŠ¡
            docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d app
            
            # ğŸ§¹ æ¸…ç†æ—§é•œåƒ
            docker image prune -f
            
            # ğŸ¥ å¥åº·æ£€æŸ¥
            sleep 30
            curl -f http://localhost/health || exit 1
            
            echo "âœ… Deployment completed successfully!"
```

## ğŸ“‹ éƒ¨ç½²è„šæœ¬

### ä¸€é”®éƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# scripts/deploy.sh - Contabo VPS ä¸€é”®éƒ¨ç½²è„šæœ¬

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

echo "ğŸš€ Starting KNZN deployment to Contabo VPS..."

# ğŸ“‹ é…ç½®å˜é‡
PROJECT_DIR="/opt/knzn"
BACKUP_DIR="/opt/knzn-backups"
LOG_FILE="/var/log/knzn-deploy.log"

# ğŸ“ æ—¥å¿—å‡½æ•°
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a $LOG_FILE
}

# ğŸš¨ é”™è¯¯å¤„ç†
error_exit() {
    log "ERROR: $1"
    exit 1
}

# ğŸ” ç¯å¢ƒæ£€æŸ¥
log "Checking environment..."

# æ£€æŸ¥ Docker
if ! command -v docker &> /dev/null; then
    error_exit "Docker is not installed"
fi

# æ£€æŸ¥ Docker Compose
if ! command -v docker-compose &> /dev/null; then
    error_exit "Docker Compose is not installed"
fi

# æ£€æŸ¥é¡¹ç›®ç›®å½•
if [ ! -d "$PROJECT_DIR" ]; then
    error_exit "Project directory $PROJECT_DIR does not exist"
fi

cd $PROJECT_DIR

# ğŸ“¥ æ›´æ–°ä»£ç 
log "Updating code from Git..."
git pull origin main || error_exit "Git pull failed"

# ğŸ” æ£€æŸ¥ç¯å¢ƒå˜é‡æ–‡ä»¶
if [ ! -f ".env.production" ]; then
    error_exit ".env.production file not found"
fi

# ğŸ’¾ åˆ›å»ºå¤‡ä»½
log "Creating backup..."
mkdir -p $BACKUP_DIR
BACKUP_NAME="knzn-backup-$(date +%Y%m%d_%H%M%S)"

# å¤‡ä»½æ•°æ®åº“
docker-compose exec -T postgres pg_dump -U knzn_user knzn_production > $BACKUP_DIR/$BACKUP_NAME.sql || error_exit "Database backup failed"

# å¤‡ä»½é…ç½®æ–‡ä»¶
cp .env.production $BACKUP_DIR/$BACKUP_NAME.env

log "Backup created: $BACKUP_NAME"

# ğŸ—ï¸ æ„å»ºå’Œéƒ¨ç½²
log "Building and deploying containers..."

# æ‹‰å–æœ€æ–°é•œåƒ
docker-compose -f docker-compose.yml -f docker-compose.prod.yml pull || error_exit "Docker pull failed"

# é‡æ–°æ„å»ºåº”ç”¨é•œåƒ (å¦‚æœéœ€è¦)
docker-compose -f docker-compose.yml -f docker-compose.prod.yml build app || error_exit "Docker build failed"

# å¯åŠ¨æœåŠ¡
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d || error_exit "Docker compose up failed"

# â±ï¸ ç­‰å¾…æœåŠ¡å¯åŠ¨
log "Waiting for services to start..."
sleep 30

# ğŸ¥ å¥åº·æ£€æŸ¥
log "Performing health checks..."

# æ£€æŸ¥ Nginx
if ! curl -f http://localhost/health > /dev/null 2>&1; then
    error_exit "Nginx health check failed"
fi

# æ£€æŸ¥åº”ç”¨
if ! curl -f http://localhost:3000/api/health > /dev/null 2>&1; then
    error_exit "App health check failed"
fi

# æ£€æŸ¥æ•°æ®åº“
if ! docker-compose exec -T postgres pg_isready -U knzn_user > /dev/null 2>&1; then
    error_exit "Database health check failed"
fi

# æ£€æŸ¥ Redis
if ! docker-compose exec -T redis redis-cli ping > /dev/null 2>&1; then
    error_exit "Redis health check failed"
fi

# ğŸ§¹ æ¸…ç†
log "Cleaning up..."

# æ¸…ç†æ—§é•œåƒ
docker image prune -f

# æ¸…ç†æ—§å¤‡ä»½ (ä¿ç•™ 7 å¤©)
find $BACKUP_DIR -name "knzn-backup-*" -mtime +7 -delete

# ğŸ“Š éƒ¨ç½²ç»Ÿè®¡
log "Deployment completed successfully!"
log "Services status:"
docker-compose -f docker-compose.yml -f docker-compose.prod.yml ps

# ğŸ“ˆ å‘é€éƒ¨ç½²é€šçŸ¥ (å¯é€‰)
if [ -n "$WEBHOOK_URL" ]; then
    curl -X POST "$WEBHOOK_URL" \
        -H "Content-Type: application/json" \
        -d "{\"text\":\"âœ… KNZN deployed successfully to Contabo VPS\"}"
fi

echo "ğŸ‰ Deployment completed successfully!"
echo "ğŸŒ Site: https://knzn.net"
echo "ğŸ“Š Logs: docker-compose logs -f"
```

### VPS åˆå§‹åŒ–è„šæœ¬

```bash
#!/bin/bash
# scripts/vps-setup.sh - Contabo VPS åˆå§‹åŒ–è„šæœ¬

set -e

echo "ğŸ–¥ï¸ Setting up Contabo VPS for KNZN..."

# 1. ç³»ç»Ÿæ›´æ–°
sudo apt update && sudo apt upgrade -y

# 2. å®‰è£…åŸºç¡€è½¯ä»¶
sudo apt install -y \
    curl \
    wget \
    git \
    htop \
    ufw \
    fail2ban \
    unzip \
    software-properties-common \
    apt-transport-https \
    ca-certificates \
    gnupg \
    lsb-release

# 3. å®‰è£… Docker
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg

echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io

# 4. å®‰è£… Docker Compose
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

# 5. é…ç½® Docker ç”¨æˆ·ç»„
sudo usermod -aG docker $USER

# 6. é…ç½®é˜²ç«å¢™
sudo ufw default deny incoming
sudo ufw default allow outgoing
sudo ufw allow ssh
sudo ufw allow 80
sudo ufw allow 443
sudo ufw --force enable

# 7. é…ç½® Fail2Ban
sudo systemctl enable fail2ban
sudo systemctl start fail2ban

# 8. åˆ›å»ºé¡¹ç›®ç›®å½•
sudo mkdir -p /opt/knzn
sudo chown $USER:$USER /opt/knzn

# 9. åˆ›å»ºå¤‡ä»½ç›®å½•
sudo mkdir -p /opt/knzn-backups
sudo chown $USER:$USER /opt/knzn-backups

# 10. é…ç½®ç³»ç»Ÿä¼˜åŒ–
echo "# KNZN System Optimization" | sudo tee -a /etc/sysctl.conf
echo "vm.max_map_count=262144" | sudo tee -a /etc/sysctl.conf
echo "fs.file-max=65536" | sudo tee -a /etc/sysctl.conf
echo "net.core.somaxconn=65535" | sudo tee -a /etc/sysctl.conf

# 11. åº”ç”¨ç³»ç»Ÿé…ç½®
sudo sysctl -p

# 12. é…ç½® Docker å®ˆæŠ¤è¿›ç¨‹
sudo tee /etc/docker/daemon.json << EOF
{
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3"
  },
  "storage-driver": "overlay2"
}
EOF

# 13. é‡å¯ Docker
sudo systemctl restart docker
sudo systemctl enable docker

# 14. å®‰è£… Node.js (ç”¨äºæœ¬åœ°å¼€å‘)
curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
sudo apt-get install -y nodejs

# 15. å®‰è£… pnpm
npm install -g pnpm

echo "âœ… Contabo VPS setup completed!"
echo "ğŸ“‹ Next steps:"
echo "1. Clone your repository to /opt/knzn"
echo "2. Configure .env.production file"
echo "3. Run ./scripts/deploy.sh"
echo ""
echo "ğŸ”„ Please log out and log back in to apply Docker group changes"
```

## ğŸ’¾ å¤‡ä»½ä¸æ¢å¤ç³»ç»Ÿ

### è‡ªåŠ¨å¤‡ä»½è„šæœ¬

```bash
#!/bin/bash
# scripts/backup.sh - å¢å¼ºå¤‡ä»½è„šæœ¬

set -e

# ğŸ“‹ é…ç½®å˜é‡
BACKUP_DIR="/opt/knzn-backups"
PROJECT_DIR="/opt/knzn"
LOG_FILE="/var/log/knzn-backup.log"
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_NAME="knzn-backup-${DATE}"
RETENTION_DAYS=30

# ğŸ“ æ—¥å¿—å‡½æ•°
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a $LOG_FILE
}

# ğŸš¨ é”™è¯¯å¤„ç†
error_exit() {
    log "ERROR: $1"
    # å‘é€å‘Šè­¦é‚®ä»¶ (å¦‚æœé…ç½®äº†)
    if [ -n "$ALERT_EMAIL" ]; then
        echo "Backup failed: $1" | mail -s "KNZN Backup Failed" $ALERT_EMAIL
    fi
    exit 1
}

log "Starting KNZN backup: $BACKUP_NAME"

# ğŸ—‚ï¸ åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR || error_exit "Failed to create backup directory"

cd $PROJECT_DIR

# ğŸ—„ï¸ æ•°æ®åº“å¤‡ä»½
log "Creating database backup..."
docker-compose exec -T postgres pg_dump -U knzn_user -c knzn_production > $BACKUP_DIR/${BACKUP_NAME}.sql || error_exit "Database backup failed"

# ğŸ“ æ£€æŸ¥å¤‡ä»½æ–‡ä»¶å¤§å°
BACKUP_SIZE=$(stat -c%s "$BACKUP_DIR/${BACKUP_NAME}.sql")
if [ $BACKUP_SIZE -lt 1000 ]; then
    error_exit "Backup file too small: ${BACKUP_SIZE} bytes"
fi

log "Database backup completed: ${BACKUP_SIZE} bytes"

# ğŸ—œï¸ å‹ç¼©å¤‡ä»½
log "Compressing backup..."
gzip $BACKUP_DIR/${BACKUP_NAME}.sql || error_exit "Compression failed"

# ğŸ” åŠ å¯†å¤‡ä»½ (å¦‚æœé…ç½®äº†åŠ å¯†å¯†é’¥)
if [ -n "$BACKUP_ENCRYPTION_KEY" ]; then
    log "Encrypting backup..."
    openssl enc -aes-256-cbc -salt \
        -in $BACKUP_DIR/${BACKUP_NAME}.sql.gz \
        -out $BACKUP_DIR/${BACKUP_NAME}.sql.gz.enc \
        -k $BACKUP_ENCRYPTION_KEY || error_exit "Encryption failed"
    
    # åˆ é™¤æœªåŠ å¯†æ–‡ä»¶
    rm $BACKUP_DIR/${BACKUP_NAME}.sql.gz
    FINAL_FILE="${BACKUP_NAME}.sql.gz.enc"
else
    FINAL_FILE="${BACKUP_NAME}.sql.gz"
fi

# ğŸ“¤ ä¸Šä¼ åˆ°äº‘å­˜å‚¨ (å¦‚æœé…ç½®äº†)
if [ -n "$R2_ACCESS_KEY_ID" ] && [ -n "$R2_SECRET_ACCESS_KEY" ]; then
    log "Uploading to Cloudflare R2..."
    
    # é…ç½® AWS CLI for R2
    export AWS_ACCESS_KEY_ID=$R2_ACCESS_KEY_ID
    export AWS_SECRET_ACCESS_KEY=$R2_SECRET_ACCESS_KEY
    
    aws s3 cp $BACKUP_DIR/$FINAL_FILE \
        s3://knzn-backups/database/$FINAL_FILE \
        --endpoint-url https://$CLOUDFLARE_ACCOUNT_ID.r2.cloudflarestorage.com || error_exit "Upload to R2 failed"
    
    log "Backup uploaded to R2: $FINAL_FILE"
fi

# ğŸ“‹ å¤‡ä»½é…ç½®æ–‡ä»¶
log "Backing up configuration files..."
cp .env.production $BACKUP_DIR/${BACKUP_NAME}.env
cp docker-compose.yml $BACKUP_DIR/${BACKUP_NAME}-compose.yml
cp docker-compose.prod.yml $BACKUP_DIR/${BACKUP_NAME}-compose-prod.yml

# ğŸ—‘ï¸ æ¸…ç†æ—§å¤‡ä»½
log "Cleaning up old backups..."
find $BACKUP_DIR -name "knzn-backup-*" -mtime +$RETENTION_DAYS -delete

# ğŸ“Š å¤‡ä»½ç»Ÿè®¡
FINAL_SIZE=$(stat -c%s "$BACKUP_DIR/$FINAL_FILE")
log "Backup completed successfully!"
log "Final file: $FINAL_FILE"
log "Final size: $FINAL_SIZE bytes"

# ğŸ“ˆ å‘é€æˆåŠŸé€šçŸ¥
if [ -n "$WEBHOOK_URL" ]; then
    curl -X POST "$WEBHOOK_URL" \
        -H "Content-Type: application/json" \
        -d "{\"text\":\"âœ… KNZN backup completed: $FINAL_FILE ($FINAL_SIZE bytes)\"}"
fi

log "Backup process completed successfully!"
```

### æ¢å¤è„šæœ¬

```bash
#!/bin/bash
# scripts/restore.sh - æ•°æ®åº“æ¢å¤è„šæœ¬

set -e

# ğŸ“‹ é…ç½®
BACKUP_DIR="/opt/knzn-backups"
PROJECT_DIR="/opt/knzn"
LOG_FILE="/var/log/knzn-restore.log"

# ğŸ“ æ—¥å¿—å‡½æ•°
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a $LOG_FILE
}

# ğŸš¨ é”™è¯¯å¤„ç†
error_exit() {
    log "ERROR: $1"
    exit 1
}

# ğŸ“¥ å‚æ•°æ£€æŸ¥
if [ $# -ne 1 ]; then
    echo "Usage: $0 <backup_filename>"
    echo "Example: $0 knzn-backup-20241223_020000.sql.gz"
    echo ""
    echo "Available backups:"
    ls -la $BACKUP_DIR/knzn-backup-*.sql.gz* 2>/dev/null || echo "No backups found"
    exit 1
fi

BACKUP_FILE=$1
BACKUP_PATH="$BACKUP_DIR/$BACKUP_FILE"

# ğŸ” æ£€æŸ¥å¤‡ä»½æ–‡ä»¶
if [ ! -f "$BACKUP_PATH" ]; then
    error_exit "Backup file not found: $BACKUP_PATH"
fi

log "Starting database restore from: $BACKUP_FILE"

# âš ï¸ å®‰å…¨ç¡®è®¤
echo "âš ï¸  WARNING: This will REPLACE the current database!"
echo "Backup file: $BACKUP_FILE"
echo "Database: knzn_production"
echo ""
read -p "Are you sure you want to continue? (type 'YES' to confirm): " CONFIRM

if [ "$CONFIRM" != "YES" ]; then
    log "Restore cancelled by user"
    exit 0
fi

cd $PROJECT_DIR

# ğŸ’¾ åˆ›å»ºå½“å‰æ•°æ®åº“å¤‡ä»½ (å®‰å…¨æªæ–½)
log "Creating safety backup of current database..."
SAFETY_BACKUP="safety-backup-$(date +%Y%m%d_%H%M%S).sql"
docker-compose exec -T postgres pg_dump -U knzn_user knzn_production > $BACKUP_DIR/$SAFETY_BACKUP || error_exit "Safety backup failed"

log "Safety backup created: $SAFETY_BACKUP"

# ğŸ”“ å¤„ç†åŠ å¯†æ–‡ä»¶
RESTORE_FILE="$BACKUP_PATH"
if [[ $BACKUP_FILE == *.enc ]]; then
    if [ -z "$BACKUP_ENCRYPTION_KEY" ]; then
        error_exit "Backup is encrypted but BACKUP_ENCRYPTION_KEY is not set"
    fi
    
    log "Decrypting backup..."
    DECRYPTED_FILE="${BACKUP_PATH%.enc}"
    openssl enc -aes-256-cbc -d \
        -in $BACKUP_PATH \
        -out $DECRYPTED_FILE \
        -k $BACKUP_ENCRYPTION_KEY || error_exit "Decryption failed"
    
    RESTORE_FILE="$DECRYPTED_FILE"
fi

# ğŸ—œï¸ è§£å‹æ–‡ä»¶
if [[ $RESTORE_FILE == *.gz ]]; then
    log "Decompressing backup..."
    DECOMPRESSED_FILE="${RESTORE_FILE%.gz}"
    gunzip -c $RESTORE_FILE > $DECOMPRESSED_FILE || error_exit "Decompression failed"
    RESTORE_FILE="$DECOMPRESSED_FILE"
fi

# ğŸ”„ åœæ­¢åº”ç”¨ (é¿å…æ•°æ®åº“è¿æ¥å†²çª)
log "Stopping application..."
docker-compose stop app

# ğŸ—„ï¸ æ¢å¤æ•°æ®åº“
log "Restoring database..."
docker-compose exec -T postgres psql -U knzn_user -d knzn_production < $RESTORE_FILE || {
    log "Restore failed! Rolling back..."
    docker-compose exec -T postgres psql -U knzn_user -d knzn_production < $BACKUP_DIR/$SAFETY_BACKUP
    error_exit "Database restore failed and rolled back"
}

# ğŸš€ é‡å¯åº”ç”¨
log "Restarting application..."
docker-compose start app

# â±ï¸ ç­‰å¾…åº”ç”¨å¯åŠ¨
sleep 30

# âœ… éªŒè¯æ¢å¤
log "Verifying restore..."
if ! curl -f http://localhost:3000/api/health > /dev/null 2>&1; then
    error_exit "Application health check failed after restore"
fi

# ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶
if [ -f "$DECOMPRESSED_FILE" ]; then
    rm -f "$DECOMPRESSED_FILE"
fi

if [ -f "$DECRYPTED_FILE" ]; then
    rm -f "$DECRYPTED_FILE"
fi

log "Database restore completed successfully!"
log "Restored from: $BACKUP_FILE"

# ğŸ“ˆ å‘é€æ¢å¤é€šçŸ¥
if [ -n "$WEBHOOK_URL" ]; then
    curl -X POST "$WEBHOOK_URL" \
        -H "Content-Type: application/json" \
        -d "{\"text\":\"âœ… KNZN database restored from: $BACKUP_FILE\"}"
fi

echo "âœ… Database restore completed successfully!"
```

## ğŸ“Š ç›‘æ§ä¸å‘Šè­¦

### ç³»ç»Ÿç›‘æ§è„šæœ¬

```bash
#!/bin/bash
# scripts/monitor.sh - ç³»ç»Ÿç›‘æ§è„šæœ¬

# ğŸ“‹ é…ç½®
LOG_FILE="/var/log/knzn-monitor.log"
ALERT_THRESHOLD_CPU=80
ALERT_THRESHOLD_MEMORY=85
ALERT_THRESHOLD_DISK=90

# ğŸ“ æ—¥å¿—å‡½æ•°
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a $LOG_FILE
}

# ğŸš¨ å‘é€å‘Šè­¦
send_alert() {
    local severity=$1
    local message=$2
    local metric=$3
    local value=$4
    
    log "ALERT [$severity]: $message"
    
    # å‘é€ Webhook é€šçŸ¥
    if [ -n "$WEBHOOK_URL" ]; then
        curl -X POST "$WEBHOOK_URL" \
            -H "Content-Type: application/json" \
            -d "{
                \"text\": \"ğŸš¨ KNZN Alert [$severity]: $message\",
                \"metric\": \"$metric\",
                \"value\": \"$value\",
                \"timestamp\": \"$(date -Iseconds)\",
                \"hostname\": \"$(hostname)\"
            }"
    fi
    
    # å‘é€é‚®ä»¶å‘Šè­¦
    if [ -n "$ALERT_EMAIL" ]; then
        echo "$message (Value: $value)" | mail -s "KNZN Alert: $metric" $ALERT_EMAIL
    fi
}

log "Starting system monitoring check..."

# ğŸ’¾ æ£€æŸ¥ç£ç›˜ä½¿ç”¨ç‡
DISK_USAGE=$(df / | awk 'NR==2 {print $5}' | sed 's/%//')
if [ $DISK_USAGE -gt $ALERT_THRESHOLD_DISK ]; then
    send_alert "CRITICAL" "Disk usage critical: ${DISK_USAGE}%" "disk_usage" "$DISK_USAGE"
elif [ $DISK_USAGE -gt 75 ]; then
    send_alert "WARNING" "Disk usage high: ${DISK_USAGE}%" "disk_usage" "$DISK_USAGE"
fi

# ğŸ§  æ£€æŸ¥å†…å­˜ä½¿ç”¨ç‡
MEMORY_USAGE=$(free | awk 'NR==2{printf "%.0f", $3*100/$2}')
if [ $MEMORY_USAGE -gt $ALERT_THRESHOLD_MEMORY ]; then
    send_alert "CRITICAL" "Memory usage critical: ${MEMORY_USAGE}%" "memory_usage" "$MEMORY_USAGE"
elif [ $MEMORY_USAGE -gt 70 ]; then
    send_alert "WARNING" "Memory usage high: ${MEMORY_USAGE}%" "memory_usage" "$MEMORY_USAGE"
fi

# ğŸ”„ æ£€æŸ¥ CPU ä½¿ç”¨ç‡
CPU_USAGE=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | sed 's/%us,//')
CPU_USAGE_INT=$(echo $CPU_USAGE | cut -d'.' -f1)
if [ $CPU_USAGE_INT -gt $ALERT_THRESHOLD_CPU ]; then
    send_alert "CRITICAL" "CPU usage critical: ${CPU_USAGE}%" "cpu_usage" "$CPU_USAGE"
elif [ $CPU_USAGE_INT -gt 60 ]; then
    send_alert "WARNING" "CPU usage high: ${CPU_USAGE}%" "cpu_usage" "$CPU_USAGE"
fi

# ğŸ³ æ£€æŸ¥ Docker å®¹å™¨çŠ¶æ€
CONTAINERS_DOWN=$(docker-compose -f /opt/knzn/docker-compose.yml -f /opt/knzn/docker-compose.prod.yml ps -q | xargs docker inspect -f '{{.State.Status}}' | grep -v running | wc -l)
if [ $CONTAINERS_DOWN -gt 0 ]; then
    send_alert "CRITICAL" "Some containers are not running" "containers_down" "$CONTAINERS_DOWN"
fi

# ğŸŒ æ£€æŸ¥ç½‘ç«™å¯ç”¨æ€§
if ! curl -f -s http://localhost/health > /dev/null; then
    send_alert "CRITICAL" "Website health check failed" "website_health" "failed"
fi

# ğŸ—„ï¸ æ£€æŸ¥æ•°æ®åº“è¿æ¥
if ! docker-compose -f /opt/knzn/docker-compose.yml exec -T postgres pg_isready -U knzn_user > /dev/null 2>&1; then
    send_alert "CRITICAL" "Database connection failed" "database_connection" "failed"
fi

# ğŸ”„ æ£€æŸ¥ Redis è¿æ¥
if ! docker-compose -f /opt/knzn/docker-compose.yml exec -T redis redis-cli ping > /dev/null 2>&1; then
    send_alert "CRITICAL" "Redis connection failed" "redis_connection" "failed"
fi

log "System monitoring check completed"
```

### Cron ä»»åŠ¡é…ç½®

```bash
# æ·»åŠ åˆ° crontab: crontab -e

# ğŸ• æ¯å°æ—¶ç›‘æ§ç³»ç»ŸçŠ¶æ€
0 * * * * /opt/knzn/scripts/monitor.sh

# ğŸŒ… æ¯å¤©å‡Œæ™¨ 2 ç‚¹å¤‡ä»½æ•°æ®åº“
0 2 * * * /opt/knzn/scripts/backup.sh

# ğŸ§¹ æ¯å‘¨æ—¥å‡Œæ™¨ 4 ç‚¹æ¸…ç† Docker
0 4 * * 0 docker system prune -f

# ğŸ”„ æ¯å¤©å‡Œæ™¨ 3 ç‚¹é‡å¯ Nginx (æ¸…ç†ç¼“å­˜)
0 3 * * * docker-compose -f /opt/knzn/docker-compose.yml restart nginx

# ğŸ“Š æ¯å°æ—¶æ£€æŸ¥ SSL è¯ä¹¦æœ‰æ•ˆæœŸ
0 * * * * /opt/knzn/scripts/check-ssl.sh
```

## ğŸ’° æˆæœ¬ä¼˜åŒ– (Contabo VPS)

### æœˆåº¦æˆæœ¬åˆ†æ

```typescript
const CONTABO_MONTHLY_COSTS = {
  // ğŸ–¥ï¸ åŸºç¡€è®¾æ–½æˆæœ¬
  infrastructure: {
    contaboVPS: {
      plan: 'VPS L (12GB RAM, 6 vCPU, 100GB NVMe)',
      cost: 12.99, // â‚¬12.99/æœˆ
      description: 'å®Œæ•´åº”ç”¨æ ˆ + æ•°æ®åº“'
    },
    domain: {
      provider: 'Cloudflare',
      cost: 0, // å…è´¹åŸŸåç®¡ç†
      description: 'DNS + åŸºç¡€ CDN'
    }
  },
  
  // ğŸ› ï¸ ç¬¬ä¸‰æ–¹æœåŠ¡æˆæœ¬ (ä¿æŒä¸å˜)
  services: {
    resend: {
      plan: 'Pro',
      emails: '100K/æœˆ',
      cost: 20,
      description: 'é‚®ä»¶å‘é€æœåŠ¡'
    },
    openai: {
      model: 'gpt-4o-mini',
      tokens: '10M tokens/æœˆ',
      cost: 15,
      description: 'AI åŠ©æ•™æœåŠ¡'
    },
    cloudflareR2: {
      storage: '50GB',
      requests: '1M/æœˆ',
      cost: 8,
      description: 'æ–‡ä»¶å­˜å‚¨ + CDN'
    },
    lemonSqueezy: {
      commission: '5% + $0.50 per transaction',
      cost: 0, // ä»æ”¶å…¥ä¸­æ‰£é™¤
      description: 'æ”¯ä»˜å¤„ç†è´¹ç”¨'
    }
  },
  
  // ğŸ“Š æ€»æˆæœ¬ (å¤§å¹…é™ä½)
  totalMonthlyCost: 56, // $56/æœˆ (vs åŸæ¥ $75)
  
  // ğŸ¯ æ”¶å…¥ç›®æ ‡ (ä¸å˜)
  revenueTarget: {
    proUsers: 1000,
    pricePerUser: 9.99,
    monthlyRevenue: 9990,
    netProfit: 9934, // 99.4% åˆ©æ¶¦ç‡
    profitMargin: '99.4%'
  },
  
  // ğŸ“ˆ æ‰©å±•æˆæœ¬ (10K ç”¨æˆ·)
  scalingCosts: {
    contaboVPS: 25.99, // å‡çº§åˆ° VPS XL (16GB RAM, 8 CPU cores)
    r2: 25, // 200GB å­˜å‚¨
    resend: 40, // 500K é‚®ä»¶
    openai: 50, // 50M tokens
    total: 141 // $141/æœˆ (vs åŸæ¥ $159)
  }
}
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v2.0 - Contabo VPS ä¸“ç”¨ç‰ˆ  
**æœ€åæ›´æ–°**: 2024-12-23  
**é€‚ç”¨é¡¹ç›®**: KNZN ç¡¬ä»¶å­¦ä¹ å¹³å°  
**éƒ¨ç½²ç­–ç•¥**: Docker å®¹å™¨åŒ–é›†ç¾¤ + Nginx åä»£

è¿™ä»½è¿ç»´æŒ‡å—ä¸“ä¸º Contabo VPS çš„å•æœºå®¹å™¨åŒ–éƒ¨ç½²è®¾è®¡ï¼Œé€šè¿‡ Docker Compose ç¼–æ’å®ç°å®Œæ•´çš„åº”ç”¨æ ˆï¼Œç›¸æ¯” Vercel æ··åˆæ–¹æ¡ˆè¿›ä¸€æ­¥é™ä½äº†æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†é«˜å¯ç”¨æ€§å’Œæ˜“ç»´æŠ¤æ€§ã€‚