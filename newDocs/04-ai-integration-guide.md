# AI é›†æˆå¼€å‘æŠ€æœ¯æŒ‡å¯¼æ–‡æ¡£

## ğŸ“‹ æ–‡æ¡£æ¦‚è¿°

**é€‚ç”¨åœºæ™¯**: éœ€è¦é›†æˆ AI åŠŸèƒ½çš„åº”ç”¨é¡¹ç›®  
**æŠ€æœ¯ç‰¹ç‚¹**: RAG + LangChain + å‘é‡æ•°æ®åº“ + æµå¼å“åº”  
**å‚è€ƒé¡¹ç›®**: ç¡¬ä»¶å­¦ä¹ å¹³å°çš„ AI é—®ç­”ç³»ç»Ÿ  
**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  

## ğŸ¯ æ¶æ„æ¦‚è¿°

åŸºäºå½“å‰é¡¹ç›®éªŒè¯çš„ AI é›†æˆæ–¹æ¡ˆï¼Œæä¾›å®Œæ•´çš„ AI åº”ç”¨å¼€å‘æŒ‡å¯¼ï¼š

- **RAG æ¶æ„**: æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œæé«˜å›ç­”å‡†ç¡®æ€§
- **å¤šæ¨¡å‹æ”¯æŒ**: OpenAI GPT-4 + Anthropic Claude
- **å‘é‡æ£€ç´¢**: Qdrant å‘é‡æ•°æ®åº“ï¼Œé«˜æ•ˆè¯­ä¹‰æœç´¢
- **æµå¼å“åº”**: Server-Sent Eventsï¼Œå®æ—¶æµå¼è¾“å‡º
- **æˆæœ¬æ§åˆ¶**: æ™ºèƒ½ç¼“å­˜ + æ¨¡å‹é€‰æ‹©ç­–ç•¥

## ğŸ—ï¸ AI ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ç”¨æˆ·äº¤äº’å±‚                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å‰ç«¯ç•Œé¢ â”‚ èŠå¤©ç»„ä»¶ â”‚ æµå¼æ˜¾ç¤º â”‚ è¯­éŸ³è¾“å…¥ â”‚ æ–‡ä»¶ä¸Šä¼             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API ç½‘å…³å±‚                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ è¯·æ±‚éªŒè¯                                                      â”‚
â”‚ â€¢ ç”¨æˆ·è®¤è¯                                                      â”‚
â”‚ â€¢ é¢‘ç‡é™åˆ¶                                                      â”‚
â”‚ â€¢ æµå¼å“åº”å¤„ç†                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI æœåŠ¡ç¼–æ’å±‚ (LangChain)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ é—®é¢˜åˆ†æ â”‚ æ„å›¾è¯†åˆ« â”‚ ä¸Šä¸‹æ–‡ç®¡ç† â”‚ å·¥ä½œæµç¼–æ’ â”‚ ç»“æœåå¤„ç†      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   çŸ¥è¯†æ£€ç´¢å±‚     â”‚ â”‚   LLM æ¨ç†å±‚     â”‚ â”‚   ç¼“å­˜ç­–ç•¥å±‚     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ å‘é‡æ£€ç´¢       â”‚ â”‚ â€¢ GPT-4 Turbo    â”‚ â”‚ â€¢ Redis ç¼“å­˜     â”‚
â”‚ â€¢ è¯­ä¹‰æœç´¢       â”‚ â”‚ â€¢ Claude 3       â”‚ â”‚ â€¢ ç›¸ä¼¼é—®é¢˜åŒ¹é…   â”‚
â”‚ â€¢ ç›¸å…³æ€§æ’åº     â”‚ â”‚ â€¢ æ¨¡å‹é€‰æ‹©       â”‚ â”‚ â€¢ ç»“æœç¼“å­˜       â”‚
â”‚ â€¢ ä¸Šä¸‹æ–‡æ„å»º     â”‚ â”‚ â€¢ å‚æ•°ä¼˜åŒ–       â”‚ â”‚ â€¢ æˆæœ¬æ§åˆ¶       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Qdrant å‘é‡åº“   â”‚ â”‚   çŸ¥è¯†åº“ç®¡ç†     â”‚ â”‚   ç›‘æ§åˆ†æå±‚     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ æ–‡æ¡£å‘é‡åŒ–     â”‚ â”‚ â€¢ æ–‡æ¡£è§£æ       â”‚ â”‚ â€¢ ä½¿ç”¨ç»Ÿè®¡       â”‚
â”‚ â€¢ ç›¸ä¼¼åº¦è®¡ç®—     â”‚ â”‚ â€¢ å†…å®¹æ›´æ–°       â”‚ â”‚ â€¢ è´¨é‡è¯„ä¼°       â”‚
â”‚ â€¢ å¿«é€Ÿæ£€ç´¢       â”‚ â”‚ â€¢ ç‰ˆæœ¬ç®¡ç†       â”‚ â”‚ â€¢ æˆæœ¬åˆ†æ       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æŠ€æœ¯æ ˆé€‰å‹

#### AI æŠ€æœ¯æ ˆ
```typescript
const AI_TECH_STACK = {
  // æ ¸å¿ƒæ¡†æ¶
  orchestration: 'LangChain',
  
  // å¤§è¯­è¨€æ¨¡å‹
  llm: {
    primary: 'OpenAI GPT-4 Turbo',
    secondary: 'Anthropic Claude 3',
    embedding: 'OpenAI text-embedding-3-small'
  },
  
  // å‘é‡æ•°æ®åº“
  vectorDB: {
    primary: 'Qdrant',
    alternative: 'Pinecone'
  },
  
  // æ–‡æ¡£å¤„ç†
  documentProcessing: {
    pdf: 'pdf-parse',
    markdown: 'markdown-it',
    chunking: 'langchain/text_splitter'
  },
  
  // ç¼“å­˜å’Œä¼˜åŒ–
  caching: {
    redis: 'ioredis',
    similarity: 'cosine-similarity'
  }
}
```

## ğŸ“ é¡¹ç›®ç»“æ„

### AI ç›¸å…³ç›®å½•ç»“æ„

```
project-root/
â”œâ”€â”€ apps/backend/src/
â”‚   â”œâ”€â”€ ai/                           # AI æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ controllers/              # AI API æ§åˆ¶å™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.controller.ts    # èŠå¤©æ¥å£
â”‚   â”‚   â”‚   â”œâ”€â”€ knowledge.controller.ts # çŸ¥è¯†åº“ç®¡ç†
â”‚   â”‚   â”‚   â””â”€â”€ embedding.controller.ts # å‘é‡åŒ–æ¥å£
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ services/                 # AI æœåŠ¡å±‚
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.service.ts        # LLM æœåŠ¡
â”‚   â”‚   â”‚   â”œâ”€â”€ rag.service.ts        # RAG æœåŠ¡
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding.service.ts  # å‘é‡åŒ–æœåŠ¡
â”‚   â”‚   â”‚   â”œâ”€â”€ knowledge.service.ts  # çŸ¥è¯†åº“æœåŠ¡
â”‚   â”‚   â”‚   â””â”€â”€ cache.service.ts      # ç¼“å­˜æœåŠ¡
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ chains/                   # LangChain å·¥ä½œæµ
â”‚   â”‚   â”‚   â”œâ”€â”€ qa-chain.ts           # é—®ç­”é“¾
â”‚   â”‚   â”‚   â”œâ”€â”€ summarize-chain.ts    # æ‘˜è¦é“¾
â”‚   â”‚   â”‚   â””â”€â”€ analysis-chain.ts     # åˆ†æé“¾
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ prompts/                  # æç¤ºè¯æ¨¡æ¿
â”‚   â”‚   â”‚   â”œâ”€â”€ system-prompts.ts     # ç³»ç»Ÿæç¤ºè¯
â”‚   â”‚   â”‚   â”œâ”€â”€ qa-prompts.ts         # é—®ç­”æç¤ºè¯
â”‚   â”‚   â”‚   â””â”€â”€ context-prompts.ts    # ä¸Šä¸‹æ–‡æç¤ºè¯
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ utils/                    # AI å·¥å…·å‡½æ•°
â”‚   â”‚   â”‚   â”œâ”€â”€ text-splitter.ts      # æ–‡æœ¬åˆ†å‰²
â”‚   â”‚   â”‚   â”œâ”€â”€ similarity.ts         # ç›¸ä¼¼åº¦è®¡ç®—
â”‚   â”‚   â”‚   â””â”€â”€ token-counter.ts      # Token è®¡æ•°
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ types/                    # AI ç±»å‹å®šä¹‰
â”‚   â”‚       â”œâ”€â”€ llm.types.ts          # LLM ç±»å‹
â”‚   â”‚       â”œâ”€â”€ rag.types.ts          # RAG ç±»å‹
â”‚   â”‚       â””â”€â”€ knowledge.types.ts    # çŸ¥è¯†åº“ç±»å‹
â”‚   â”‚
â”‚   â””â”€â”€ knowledge/                    # çŸ¥è¯†åº“ç®¡ç†
â”‚       â”œâ”€â”€ documents/                # æ–‡æ¡£å­˜å‚¨
â”‚       â”œâ”€â”€ processors/               # æ–‡æ¡£å¤„ç†å™¨
â”‚       â””â”€â”€ indexers/                 # ç´¢å¼•å™¨
â”‚
â”œâ”€â”€ packages/ai-utils/                # AI å·¥å…·åŒ…
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ llm/                      # LLM å®¢æˆ·ç«¯
â”‚   â”‚   â”œâ”€â”€ embedding/                # å‘é‡åŒ–å·¥å…·
â”‚   â”‚   â”œâ”€â”€ rag/                      # RAG å·¥å…·
â”‚   â”‚   â””â”€â”€ cache/                    # ç¼“å­˜å·¥å…·
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ apps/web/app/
    â”œâ”€â”€ components/ai/                # AI å‰ç«¯ç»„ä»¶
    â”‚   â”œâ”€â”€ ChatInterface.vue         # èŠå¤©ç•Œé¢
    â”‚   â”œâ”€â”€ MessageBubble.vue         # æ¶ˆæ¯æ°”æ³¡
    â”‚   â”œâ”€â”€ StreamingText.vue         # æµå¼æ–‡æœ¬
    â”‚   â””â”€â”€ KnowledgeUpload.vue       # çŸ¥è¯†ä¸Šä¼ 
    â”‚
    â”œâ”€â”€ composables/
    â”‚   â”œâ”€â”€ useChat.ts                # èŠå¤©é€»è¾‘
    â”‚   â”œâ”€â”€ useStreaming.ts           # æµå¼å¤„ç†
    â”‚   â””â”€â”€ useKnowledge.ts           # çŸ¥è¯†åº“ç®¡ç†
    â”‚
    â””â”€â”€ stores/
        â”œâ”€â”€ chat.ts                   # èŠå¤©çŠ¶æ€
        â””â”€â”€ knowledge.ts              # çŸ¥è¯†åº“çŠ¶æ€
```

## ğŸ¤– æ ¸å¿ƒ AI æœåŠ¡å®ç°

### 1. LLM æœåŠ¡é…ç½®

```typescript
// apps/backend/src/ai/services/llm.service.ts
import { Injectable } from '@nestjs/common'
import { OpenAI } from 'openai'
import Anthropic from '@anthropic-ai/sdk'

@Injectable()
export class LLMService {
  private openai: OpenAI
  private anthropic: Anthropic

  constructor() {
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY
    })

    this.anthropic = new Anthropic({
      apiKey: process.env.ANTHROPIC_API_KEY
    })
  }

  async generateResponse(
    messages: Array<{ role: string; content: string }>,
    options: {
      model?: 'gpt-4' | 'claude-3'
      temperature?: number
      maxTokens?: number
      stream?: boolean
    } = {}
  ) {
    const {
      model = 'gpt-4',
      temperature = 0.7,
      maxTokens = 2000,
      stream = false
    } = options

    if (model === 'gpt-4') {
      return this.generateOpenAIResponse(messages, {
        temperature,
        maxTokens,
        stream
      })
    } else {
      return this.generateAnthropicResponse(messages, {
        temperature,
        maxTokens,
        stream
      })
    }
  }

  private async generateOpenAIResponse(
    messages: Array<{ role: string; content: string }>,
    options: { temperature: number; maxTokens: number; stream: boolean }
  ) {
    const response = await this.openai.chat.completions.create({
      model: 'gpt-4-turbo-preview',
      messages: messages as any,
      temperature: options.temperature,
      max_tokens: options.maxTokens,
      stream: options.stream
    })

    return response
  }

  private async generateAnthropicResponse(
    messages: Array<{ role: string; content: string }>,
    options: { temperature: number; maxTokens: number; stream: boolean }
  ) {
    // è½¬æ¢æ¶ˆæ¯æ ¼å¼
    const systemMessage = messages.find(m => m.role === 'system')
    const userMessages = messages.filter(m => m.role !== 'system')

    const response = await this.anthropic.messages.create({
      model: 'claude-3-sonnet-20240229',
      system: systemMessage?.content || '',
      messages: userMessages as any,
      temperature: options.temperature,
      max_tokens: options.maxTokens,
      stream: options.stream
    })

    return response
  }

  async generateEmbedding(text: string): Promise<number[]> {
    const response = await this.openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: text
    })

    return response.data[0].embedding
  }
}
```

### 2. RAG æœåŠ¡å®ç°

```typescript
// apps/backend/src/ai/services/rag.service.ts
import { Injectable } from '@nestjs/common'
import { QdrantVectorStore } from 'langchain/vectorstores/qdrant'
import { OpenAIEmbeddings } from 'langchain/embeddings/openai'
import { RetrievalQAChain } from 'langchain/chains'
import { ChatOpenAI } from 'langchain/chat_models/openai'

@Injectable()
export class RAGService {
  private vectorStore: QdrantVectorStore
  private embeddings: OpenAIEmbeddings
  private llm: ChatOpenAI

  constructor() {
    this.embeddings = new OpenAIEmbeddings({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: 'text-embedding-3-small'
    })

    this.llm = new ChatOpenAI({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: 'gpt-4-turbo-preview',
      temperature: 0.7
    })

    this.initializeVectorStore()
  }

  private async initializeVectorStore() {
    this.vectorStore = await QdrantVectorStore.fromExistingCollection(
      this.embeddings,
      {
        url: process.env.QDRANT_URL,
        apiKey: process.env.QDRANT_API_KEY,
        collectionName: 'knowledge_base'
      }
    )
  }

  async queryKnowledge(
    question: string,
    options: {
      topK?: number
      scoreThreshold?: number
      includeMetadata?: boolean
    } = {}
  ) {
    const {
      topK = 5,
      scoreThreshold = 0.7,
      includeMetadata = true
    } = options

    // 1. å‘é‡æ£€ç´¢ç›¸å…³æ–‡æ¡£
    const relevantDocs = await this.vectorStore.similaritySearchWithScore(
      question,
      topK
    )

    // 2. è¿‡æ»¤ä½ç›¸å…³æ€§æ–‡æ¡£
    const filteredDocs = relevantDocs.filter(
      ([doc, score]) => score >= scoreThreshold
    )

    if (filteredDocs.length === 0) {
      return {
        answer: 'æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚',
        sources: [],
        confidence: 0
      }
    }

    // 3. æ„å»ºä¸Šä¸‹æ–‡
    const context = filteredDocs
      .map(([doc, score]) => doc.pageContent)
      .join('\n\n')

    // 4. ç”Ÿæˆå›ç­”
    const chain = RetrievalQAChain.fromLLM(this.llm, this.vectorStore, {
      returnSourceDocuments: true
    })

    const response = await chain.call({
      query: question,
      context: context
    })

    return {
      answer: response.text,
      sources: filteredDocs.map(([doc, score]) => ({
        content: doc.pageContent,
        metadata: includeMetadata ? doc.metadata : {},
        score: score
      })),
      confidence: this.calculateConfidence(filteredDocs)
    }
  }

  private calculateConfidence(docs: Array<[any, number]>): number {
    if (docs.length === 0) return 0
    
    const avgScore = docs.reduce((sum, [, score]) => sum + score, 0) / docs.length
    return Math.min(avgScore, 1.0)
  }

  async addDocuments(documents: Array<{
    content: string
    metadata: Record<string, any>
  }>) {
    const docs = documents.map(doc => ({
      pageContent: doc.content,
      metadata: doc.metadata
    }))

    await this.vectorStore.addDocuments(docs)
  }
}
```

### 3. æµå¼å“åº”å®ç°

```typescript
// apps/backend/src/ai/controllers/chat.controller.ts
import { Controller, Post, Body, Sse, MessageEvent } from '@nestjs/common'
import { Observable } from 'rxjs'
import { RAGService } from '../services/rag.service'
import { LLMService } from '../services/llm.service'

@Controller('ai/chat')
export class ChatController {
  constructor(
    private ragService: RAGService,
    private llmService: LLMService
  ) {}

  @Sse('stream')
  async streamChat(
    @Body() body: { question: string; conversationId?: string }
  ): Promise<Observable<MessageEvent>> {
    return new Observable(observer => {
      this.processStreamingChat(body, observer)
    })
  }

  private async processStreamingChat(
    body: { question: string; conversationId?: string },
    observer: any
  ) {
    try {
      // 1. å‘é€å¼€å§‹ä¿¡å·
      observer.next({
        data: JSON.stringify({
          type: 'start',
          message: 'æ­£åœ¨æ€è€ƒä¸­...'
        })
      })

      // 2. æ£€ç´¢ç›¸å…³çŸ¥è¯†
      observer.next({
        data: JSON.stringify({
          type: 'searching',
          message: 'æ­£åœ¨æœç´¢ç›¸å…³çŸ¥è¯†...'
        })
      })

      const ragResult = await this.ragService.queryKnowledge(body.question)

      // 3. æ„å»ºæç¤ºè¯
      const systemPrompt = this.buildSystemPrompt(ragResult.sources)
      const messages = [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: body.question }
      ]

      // 4. æµå¼ç”Ÿæˆå›ç­”
      observer.next({
        data: JSON.stringify({
          type: 'generating',
          message: 'æ­£åœ¨ç”Ÿæˆå›ç­”...'
        })
      })

      const stream = await this.llmService.generateResponse(messages, {
        stream: true
      })

      let fullResponse = ''

      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content || ''
        if (content) {
          fullResponse += content
          observer.next({
            data: JSON.stringify({
              type: 'content',
              content: content,
              fullContent: fullResponse
            })
          })
        }
      }

      // 5. å‘é€å®Œæˆä¿¡å·
      observer.next({
        data: JSON.stringify({
          type: 'complete',
          sources: ragResult.sources,
          confidence: ragResult.confidence
        })
      })

      observer.complete()
    } catch (error) {
      observer.next({
        data: JSON.stringify({
          type: 'error',
          message: 'ç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯'
        })
      })
      observer.error(error)
    }
  }

  private buildSystemPrompt(sources: any[]): string {
    const contextText = sources
      .map(source => source.content)
      .join('\n\n')

    return `
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¡¬ä»¶å·¥ç¨‹å¸ˆå­¦ä¹ åŠ©æ•™ï¼Œæ“…é•¿æ•°å­—ç”µè·¯ã€æ¨¡æ‹Ÿç”µè·¯å’ŒåµŒå…¥å¼ç³»ç»Ÿã€‚

è¯·åŸºäºä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

${contextText}

å›ç­”è¦æ±‚ï¼š
1. åŸºäºæä¾›çš„çŸ¥è¯†åº“å†…å®¹å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šå¤æ‚æ¦‚å¿µ
3. æä¾›å®ä¾‹å’Œç±»æ¯”å¸®åŠ©ç†è§£
4. å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯šå®è¯´"ä¸çŸ¥é“"
5. å›ç­”è¦å‡†ç¡®ã€æœ‰æ¡ç†ã€æ˜“äºç†è§£

è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚
    `.trim()
  }
}
```

## ğŸ¨ å‰ç«¯ AI ç»„ä»¶

### 1. èŠå¤©ç•Œé¢ç»„ä»¶

```vue
<!-- apps/web/app/components/ai/ChatInterface.vue -->
<template>
  <div class="flex flex-col h-full bg-gray-50">
    <!-- æ¶ˆæ¯åˆ—è¡¨ -->
    <div ref="messagesContainer" class="flex-1 overflow-y-auto p-4 space-y-4">
      <div
        v-for="message in messages"
        :key="message.id"
        class="flex"
        :class="message.role === 'user' ? 'justify-end' : 'justify-start'"
      >
        <MessageBubble
          :message="message"
          :is-streaming="message.id === streamingMessageId"
        />
      </div>
      
      <!-- åŠ è½½æŒ‡ç¤ºå™¨ -->
      <div v-if="isLoading" class="flex justify-start">
        <div class="bg-white rounded-lg p-4 shadow-sm max-w-xs">
          <div class="flex items-center space-x-2">
            <div class="animate-spin rounded-full h-4 w-4 border-b-2 border-blue-500"></div>
            <span class="text-sm text-gray-600">{{ loadingMessage }}</span>
          </div>
        </div>
      </div>
    </div>

    <!-- è¾“å…¥åŒºåŸŸ -->
    <div class="border-t bg-white p-4">
      <form @submit.prevent="sendMessage" class="flex space-x-2">
        <input
          v-model="inputMessage"
          type="text"
          placeholder="è¾“å…¥ä½ çš„é—®é¢˜..."
          class="flex-1 border border-gray-300 rounded-lg px-4 py-2 focus:outline-none focus:ring-2 focus:ring-blue-500"
          :disabled="isLoading"
        >
        <button
          type="submit"
          :disabled="!inputMessage.trim() || isLoading"
          class="bg-blue-500 text-white px-6 py-2 rounded-lg hover:bg-blue-600 disabled:opacity-50 disabled:cursor-not-allowed"
        >
          <Icon v-if="isLoading" name="heroicons:arrow-path" class="animate-spin h-5 w-5" />
          <Icon v-else name="heroicons:paper-airplane" class="h-5 w-5" />
        </button>
      </form>
    </div>
  </div>
</template>

<script setup lang="ts">
interface Message {
  id: string
  role: 'user' | 'assistant'
  content: string
  timestamp: Date
  sources?: Array<{
    content: string
    metadata: Record<string, any>
    score: number
  }>
  confidence?: number
}

const messages = ref<Message[]>([])
const inputMessage = ref('')
const isLoading = ref(false)
const loadingMessage = ref('')
const streamingMessageId = ref<string | null>(null)
const messagesContainer = ref<HTMLElement>()

const { streamChat } = useChat()

const sendMessage = async () => {
  if (!inputMessage.value.trim() || isLoading.value) return

  const userMessage: Message = {
    id: generateId(),
    role: 'user',
    content: inputMessage.value,
    timestamp: new Date()
  }

  messages.value.push(userMessage)
  
  const assistantMessage: Message = {
    id: generateId(),
    role: 'assistant',
    content: '',
    timestamp: new Date()
  }

  messages.value.push(assistantMessage)
  streamingMessageId.value = assistantMessage.id

  const question = inputMessage.value
  inputMessage.value = ''
  isLoading.value = true

  try {
    await streamChat(question, {
      onStart: (message: string) => {
        loadingMessage.value = message
      },
      onSearching: (message: string) => {
        loadingMessage.value = message
      },
      onGenerating: (message: string) => {
        loadingMessage.value = message
        isLoading.value = false
      },
      onContent: (content: string, fullContent: string) => {
        assistantMessage.content = fullContent
        scrollToBottom()
      },
      onComplete: (sources: any[], confidence: number) => {
        assistantMessage.sources = sources
        assistantMessage.confidence = confidence
        streamingMessageId.value = null
        scrollToBottom()
      },
      onError: (error: string) => {
        assistantMessage.content = 'æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚'
        streamingMessageId.value = null
        isLoading.value = false
      }
    })
  } catch (error) {
    console.error('Chat error:', error)
    assistantMessage.content = 'æŠ±æ­‰ï¼Œå‘é€æ¶ˆæ¯æ—¶å‡ºç°é”™è¯¯ã€‚'
    streamingMessageId.value = null
  } finally {
    isLoading.value = false
  }
}

const scrollToBottom = () => {
  nextTick(() => {
    if (messagesContainer.value) {
      messagesContainer.value.scrollTop = messagesContainer.value.scrollHeight
    }
  })
}

const generateId = () => {
  return Math.random().toString(36).substr(2, 9)
}
</script>
```

### 2. æµå¼æ–‡æœ¬ç»„ä»¶

```vue
<!-- apps/web/app/components/ai/StreamingText.vue -->
<template>
  <div class="streaming-text">
    <span v-html="formattedContent"></span>
    <span v-if="isStreaming" class="cursor animate-pulse">|</span>
  </div>
</template>

<script setup lang="ts">
interface Props {
  content: string
  isStreaming: boolean
  typewriterEffect?: boolean
  speed?: number
}

const props = withDefaults(defineProps<Props>(), {
  typewriterEffect: false,
  speed: 50
})

const displayedContent = ref('')
const formattedContent = computed(() => {
  // ç®€å•çš„ Markdown æ¸²æŸ“
  return props.content
    .replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>')
    .replace(/\*(.*?)\*/g, '<em>$1</em>')
    .replace(/`(.*?)`/g, '<code class="bg-gray-100 px-1 rounded">$1</code>')
    .replace(/\n/g, '<br>')
})

// æ‰“å­—æœºæ•ˆæœ
watch(() => props.content, (newContent) => {
  if (props.typewriterEffect && props.isStreaming) {
    let index = displayedContent.value.length
    const timer = setInterval(() => {
      if (index < newContent.length) {
        displayedContent.value = newContent.slice(0, index + 1)
        index++
      } else {
        clearInterval(timer)
      }
    }, props.speed)
  } else {
    displayedContent.value = newContent
  }
}, { immediate: true })
</script>

<style scoped>
.streaming-text {
  line-height: 1.6;
}

.cursor {
  color: #3b82f6;
  font-weight: bold;
}

:deep(code) {
  font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
  font-size: 0.9em;
}
</style>
```

### 3. èŠå¤© Composable

```typescript
// apps/web/app/composables/useChat.ts
export const useChat = () => {
  const isConnected = ref(false)
  const eventSource = ref<EventSource | null>(null)

  const streamChat = async (
    question: string,
    callbacks: {
      onStart?: (message: string) => void
      onSearching?: (message: string) => void
      onGenerating?: (message: string) => void
      onContent?: (content: string, fullContent: string) => void
      onComplete?: (sources: any[], confidence: number) => void
      onError?: (error: string) => void
    }
  ) => {
    return new Promise<void>((resolve, reject) => {
      // åˆ›å»º EventSource è¿æ¥
      const url = `/api/ai/chat/stream`
      eventSource.value = new EventSource(url)

      // å‘é€é—®é¢˜æ•°æ®
      $fetch('/api/ai/chat/stream', {
        method: 'POST',
        body: { question }
      }).catch(reject)

      eventSource.value.onopen = () => {
        isConnected.value = true
      }

      eventSource.value.onmessage = (event) => {
        try {
          const data = JSON.parse(event.data)

          switch (data.type) {
            case 'start':
              callbacks.onStart?.(data.message)
              break
            case 'searching':
              callbacks.onSearching?.(data.message)
              break
            case 'generating':
              callbacks.onGenerating?.(data.message)
              break
            case 'content':
              callbacks.onContent?.(data.content, data.fullContent)
              break
            case 'complete':
              callbacks.onComplete?.(data.sources, data.confidence)
              closeConnection()
              resolve()
              break
            case 'error':
              callbacks.onError?.(data.message)
              closeConnection()
              reject(new Error(data.message))
              break
          }
        } catch (error) {
          console.error('Parse SSE data error:', error)
          callbacks.onError?.('æ•°æ®è§£æé”™è¯¯')
          closeConnection()
          reject(error)
        }
      }

      eventSource.value.onerror = (error) => {
        console.error('SSE connection error:', error)
        callbacks.onError?.('è¿æ¥é”™è¯¯')
        closeConnection()
        reject(error)
      }
    })
  }

  const closeConnection = () => {
    if (eventSource.value) {
      eventSource.value.close()
      eventSource.value = null
      isConnected.value = false
    }
  }

  onUnmounted(() => {
    closeConnection()
  })

  return {
    streamChat,
    isConnected: readonly(isConnected),
    closeConnection
  }
}
```

## ğŸ“š çŸ¥è¯†åº“ç®¡ç†

### 1. æ–‡æ¡£å¤„ç†æœåŠ¡

```typescript
// apps/backend/src/ai/services/knowledge.service.ts
import { Injectable } from '@nestjs/common'
import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter'
import * as pdfParse from 'pdf-parse'
import * as fs from 'fs'

@Injectable()
export class KnowledgeService {
  private textSplitter: RecursiveCharacterTextSplitter

  constructor() {
    this.textSplitter = new RecursiveCharacterTextSplitter({
      chunkSize: 1000,
      chunkOverlap: 200,
      separators: ['\n\n', '\n', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', ' ', '']
    })
  }

  async processDocument(
    filePath: string,
    metadata: Record<string, any> = {}
  ) {
    const fileExtension = filePath.split('.').pop()?.toLowerCase()
    let content: string

    switch (fileExtension) {
      case 'pdf':
        content = await this.processPDF(filePath)
        break
      case 'md':
      case 'txt':
        content = await this.processText(filePath)
        break
      default:
        throw new Error(`ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: ${fileExtension}`)
    }

    // åˆ†å‰²æ–‡æ¡£
    const chunks = await this.textSplitter.splitText(content)

    // ä¸ºæ¯ä¸ªå—æ·»åŠ å…ƒæ•°æ®
    const documents = chunks.map((chunk, index) => ({
      content: chunk,
      metadata: {
        ...metadata,
        source: filePath,
        chunkIndex: index,
        totalChunks: chunks.length,
        processedAt: new Date().toISOString()
      }
    }))

    return documents
  }

  private async processPDF(filePath: string): Promise<string> {
    const buffer = fs.readFileSync(filePath)
    const data = await pdfParse(buffer)
    return data.text
  }

  private async processText(filePath: string): Promise<string> {
    return fs.readFileSync(filePath, 'utf-8')
  }

  async addDocumentsToVectorStore(
    documents: Array<{
      content: string
      metadata: Record<string, any>
    }>
  ) {
    // è¿™é‡Œè°ƒç”¨ RAG æœåŠ¡æ·»åŠ æ–‡æ¡£
    const ragService = new (await import('./rag.service')).RAGService()
    await ragService.addDocuments(documents)
  }

  async updateKnowledgeBase(filePaths: string[]) {
    const allDocuments = []

    for (const filePath of filePaths) {
      try {
        const documents = await this.processDocument(filePath, {
          filename: filePath.split('/').pop(),
          uploadedAt: new Date().toISOString()
        })
        allDocuments.push(...documents)
      } catch (error) {
        console.error(`å¤„ç†æ–‡ä»¶å¤±è´¥: ${filePath}`, error)
      }
    }

    if (allDocuments.length > 0) {
      await this.addDocumentsToVectorStore(allDocuments)
    }

    return {
      processedFiles: filePaths.length,
      totalDocuments: allDocuments.length
    }
  }
}
```

### 2. çŸ¥è¯†åº“ä¸Šä¼ ç»„ä»¶

```vue
<!-- apps/web/app/components/ai/KnowledgeUpload.vue -->
<template>
  <div class="knowledge-upload">
    <div class="upload-area" :class="{ 'drag-over': isDragOver }">
      <input
        ref="fileInput"
        type="file"
        multiple
        accept=".pdf,.md,.txt"
        @change="handleFileSelect"
        class="hidden"
      >
      
      <div
        @drop="handleDrop"
        @dragover.prevent="isDragOver = true"
        @dragleave="isDragOver = false"
        @click="$refs.fileInput.click()"
        class="border-2 border-dashed border-gray-300 rounded-lg p-8 text-center cursor-pointer hover:border-blue-400 transition-colors"
      >
        <Icon name="heroicons:cloud-arrow-up" class="mx-auto h-12 w-12 text-gray-400" />
        <p class="mt-2 text-sm text-gray-600">
          ç‚¹å‡»æˆ–æ‹–æ‹½æ–‡ä»¶åˆ°æ­¤å¤„ä¸Šä¼ 
        </p>
        <p class="text-xs text-gray-500">
          æ”¯æŒ PDFã€Markdownã€æ–‡æœ¬æ–‡ä»¶
        </p>
      </div>
    </div>

    <!-- ä¸Šä¼ è¿›åº¦ -->
    <div v-if="uploadingFiles.length > 0" class="mt-4 space-y-2">
      <div
        v-for="file in uploadingFiles"
        :key="file.id"
        class="flex items-center justify-between p-3 bg-gray-50 rounded-lg"
      >
        <div class="flex items-center space-x-3">
          <Icon name="heroicons:document-text" class="h-5 w-5 text-gray-400" />
          <span class="text-sm font-medium">{{ file.name }}</span>
        </div>
        
        <div class="flex items-center space-x-2">
          <div v-if="file.status === 'uploading'" class="flex items-center space-x-2">
            <div class="w-32 bg-gray-200 rounded-full h-2">
              <div
                class="bg-blue-600 h-2 rounded-full transition-all duration-300"
                :style="{ width: `${file.progress}%` }"
              ></div>
            </div>
            <span class="text-xs text-gray-500">{{ file.progress }}%</span>
          </div>
          
          <Icon
            v-else-if="file.status === 'completed'"
            name="heroicons:check-circle"
            class="h-5 w-5 text-green-500"
          />
          
          <Icon
            v-else-if="file.status === 'error'"
            name="heroicons:x-circle"
            class="h-5 w-5 text-red-500"
          />
        </div>
      </div>
    </div>

    <!-- å·²ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨ -->
    <div v-if="uploadedFiles.length > 0" class="mt-6">
      <h3 class="text-lg font-medium mb-3">çŸ¥è¯†åº“æ–‡ä»¶</h3>
      <div class="space-y-2">
        <div
          v-for="file in uploadedFiles"
          :key="file.id"
          class="flex items-center justify-between p-3 bg-white border rounded-lg"
        >
          <div class="flex items-center space-x-3">
            <Icon name="heroicons:document-text" class="h-5 w-5 text-blue-500" />
            <div>
              <p class="text-sm font-medium">{{ file.name }}</p>
              <p class="text-xs text-gray-500">
                {{ file.chunks }} ä¸ªæ–‡æ¡£å— â€¢ {{ formatDate(file.uploadedAt) }}
              </p>
            </div>
          </div>
          
          <button
            @click="deleteFile(file.id)"
            class="text-red-500 hover:text-red-700"
          >
            <Icon name="heroicons:trash" class="h-4 w-4" />
          </button>
        </div>
      </div>
    </div>
  </div>
</template>

<script setup lang="ts">
interface UploadingFile {
  id: string
  name: string
  file: File
  status: 'uploading' | 'completed' | 'error'
  progress: number
}

interface UploadedFile {
  id: string
  name: string
  chunks: number
  uploadedAt: Date
}

const isDragOver = ref(false)
const uploadingFiles = ref<UploadingFile[]>([])
const uploadedFiles = ref<UploadedFile[]>([])

const { uploadKnowledgeFile, getKnowledgeFiles, deleteKnowledgeFile } = useKnowledge()

const handleFileSelect = (event: Event) => {
  const files = (event.target as HTMLInputElement).files
  if (files) {
    handleFiles(Array.from(files))
  }
}

const handleDrop = (event: DragEvent) => {
  event.preventDefault()
  isDragOver.value = false
  
  const files = event.dataTransfer?.files
  if (files) {
    handleFiles(Array.from(files))
  }
}

const handleFiles = async (files: File[]) => {
  for (const file of files) {
    const uploadingFile: UploadingFile = {
      id: generateId(),
      name: file.name,
      file,
      status: 'uploading',
      progress: 0
    }
    
    uploadingFiles.value.push(uploadingFile)
    
    try {
      await uploadKnowledgeFile(file, {
        onProgress: (progress: number) => {
          uploadingFile.progress = progress
        }
      })
      
      uploadingFile.status = 'completed'
      
      // åˆ·æ–°æ–‡ä»¶åˆ—è¡¨
      await loadKnowledgeFiles()
    } catch (error) {
      uploadingFile.status = 'error'
      console.error('Upload error:', error)
    }
  }
  
  // æ¸…ç†å·²å®Œæˆçš„ä¸Šä¼ 
  setTimeout(() => {
    uploadingFiles.value = uploadingFiles.value.filter(
      file => file.status === 'uploading'
    )
  }, 2000)
}

const deleteFile = async (fileId: string) => {
  try {
    await deleteKnowledgeFile(fileId)
    await loadKnowledgeFiles()
  } catch (error) {
    console.error('Delete error:', error)
  }
}

const loadKnowledgeFiles = async () => {
  try {
    uploadedFiles.value = await getKnowledgeFiles()
  } catch (error) {
    console.error('Load files error:', error)
  }
}

const formatDate = (date: Date) => {
  return new Intl.DateTimeFormat('zh-CN', {
    year: 'numeric',
    month: 'short',
    day: 'numeric',
    hour: '2-digit',
    minute: '2-digit'
  }).format(date)
}

const generateId = () => {
  return Math.random().toString(36).substr(2, 9)
}

// åˆå§‹åŒ–åŠ è½½
onMounted(() => {
  loadKnowledgeFiles()
})
</script>

<style scoped>
.drag-over {
  @apply border-blue-400 bg-blue-50;
}
</style>
```

## ğŸ”§ ç¼“å­˜å’Œä¼˜åŒ–

### 1. æ™ºèƒ½ç¼“å­˜ç­–ç•¥

```typescript
// apps/backend/src/ai/services/cache.service.ts
import { Injectable } from '@nestjs/common'
import { Redis } from 'ioredis'
import * as crypto from 'crypto'

@Injectable()
export class CacheService {
  private redis: Redis

  constructor() {
    this.redis = new Redis(process.env.REDIS_URL)
  }

  // ç”Ÿæˆç¼“å­˜é”®
  private generateCacheKey(question: string, context?: string): string {
    const content = context ? `${question}:${context}` : question
    return `ai:cache:${crypto.createHash('md5').update(content).digest('hex')}`
  }

  // æ£€æŸ¥ç›¸ä¼¼é—®é¢˜
  async findSimilarQuestion(
    question: string,
    threshold: number = 0.85
  ): Promise<{ answer: string; confidence: number } | null> {
    const questionEmbedding = await this.generateEmbedding(question)
    
    // ä»ç¼“å­˜ä¸­è·å–æ‰€æœ‰é—®é¢˜çš„å‘é‡
    const cachedQuestions = await this.redis.hgetall('ai:questions')
    
    let bestMatch = null
    let bestSimilarity = 0

    for (const [cachedQuestion, data] of Object.entries(cachedQuestions)) {
      const cachedData = JSON.parse(data)
      const similarity = this.cosineSimilarity(
        questionEmbedding,
        cachedData.embedding
      )

      if (similarity > bestSimilarity && similarity >= threshold) {
        bestSimilarity = similarity
        bestMatch = {
          answer: cachedData.answer,
          confidence: similarity
        }
      }
    }

    return bestMatch
  }

  // ç¼“å­˜é—®ç­”å¯¹
  async cacheQA(
    question: string,
    answer: string,
    embedding: number[],
    ttl: number = 86400 // 24å°æ—¶
  ) {
    const cacheKey = this.generateCacheKey(question)
    
    const data = {
      question,
      answer,
      embedding,
      timestamp: Date.now()
    }

    await Promise.all([
      // ç¼“å­˜å®Œæ•´å›ç­”
      this.redis.setex(cacheKey, ttl, JSON.stringify(data)),
      // ç¼“å­˜é—®é¢˜å‘é‡ç”¨äºç›¸ä¼¼æ€§æœç´¢
      this.redis.hset('ai:questions', question, JSON.stringify(data))
    ])
  }

  // è·å–ç¼“å­˜çš„å›ç­”
  async getCachedAnswer(question: string): Promise<string | null> {
    const cacheKey = this.generateCacheKey(question)
    const cached = await this.redis.get(cacheKey)
    
    if (cached) {
      const data = JSON.parse(cached)
      return data.answer
    }
    
    return null
  }

  // ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
  private cosineSimilarity(a: number[], b: number[]): number {
    if (a.length !== b.length) return 0

    let dotProduct = 0
    let normA = 0
    let normB = 0

    for (let i = 0; i < a.length; i++) {
      dotProduct += a[i] * b[i]
      normA += a[i] * a[i]
      normB += b[i] * b[i]
    }

    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB))
  }

  // ç”Ÿæˆå‘é‡ï¼ˆè¿™é‡Œåº”è¯¥è°ƒç”¨ LLM æœåŠ¡ï¼‰
  private async generateEmbedding(text: string): Promise<number[]> {
    // å®é™…å®ç°ä¸­åº”è¯¥è°ƒç”¨ LLMService
    // è¿™é‡Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
    return new Array(1536).fill(0).map(() => Math.random())
  }

  // æ¸…ç†è¿‡æœŸç¼“å­˜
  async cleanupExpiredCache() {
    const questions = await this.redis.hgetall('ai:questions')
    const now = Date.now()
    const expiredQuestions = []

    for (const [question, data] of Object.entries(questions)) {
      const parsedData = JSON.parse(data)
      // æ¸…ç†è¶…è¿‡7å¤©çš„ç¼“å­˜
      if (now - parsedData.timestamp > 7 * 24 * 60 * 60 * 1000) {
        expiredQuestions.push(question)
      }
    }

    if (expiredQuestions.length > 0) {
      await this.redis.hdel('ai:questions', ...expiredQuestions)
    }

    return expiredQuestions.length
  }
}
```

### 2. æˆæœ¬æ§åˆ¶ç­–ç•¥

```typescript
// apps/backend/src/ai/services/cost-control.service.ts
import { Injectable } from '@nestjs/common'

@Injectable()
export class CostControlService {
  private readonly DAILY_LIMITS = {
    free: { requests: 10, tokens: 10000 },
    pro: { requests: 100, tokens: 100000 },
    enterprise: { requests: 1000, tokens: 1000000 }
  }

  async checkUserLimits(
    userId: string,
    userPlan: 'free' | 'pro' | 'enterprise'
  ): Promise<{ allowed: boolean; remaining: any }> {
    const today = new Date().toISOString().split('T')[0]
    const key = `usage:${userId}:${today}`
    
    const usage = await this.redis.hgetall(key)
    const currentRequests = parseInt(usage.requests || '0')
    const currentTokens = parseInt(usage.tokens || '0')
    
    const limits = this.DAILY_LIMITS[userPlan]
    
    const allowed = currentRequests < limits.requests && currentTokens < limits.tokens
    
    return {
      allowed,
      remaining: {
        requests: Math.max(0, limits.requests - currentRequests),
        tokens: Math.max(0, limits.tokens - currentTokens)
      }
    }
  }

  async recordUsage(
    userId: string,
    tokens: number
  ) {
    const today = new Date().toISOString().split('T')[0]
    const key = `usage:${userId}:${today}`
    
    await Promise.all([
      this.redis.hincrby(key, 'requests', 1),
      this.redis.hincrby(key, 'tokens', tokens),
      this.redis.expire(key, 86400) // 24å°æ—¶è¿‡æœŸ
    ])
  }

  // æ™ºèƒ½æ¨¡å‹é€‰æ‹©
  selectModel(
    question: string,
    userPlan: 'free' | 'pro' | 'enterprise'
  ): 'gpt-3.5-turbo' | 'gpt-4' | 'claude-3' {
    const questionLength = question.length
    const complexity = this.analyzeComplexity(question)

    // å…è´¹ç”¨æˆ·åªèƒ½ä½¿ç”¨ GPT-3.5
    if (userPlan === 'free') {
      return 'gpt-3.5-turbo'
    }

    // æ ¹æ®é—®é¢˜å¤æ‚åº¦é€‰æ‹©æ¨¡å‹
    if (complexity > 0.8 || questionLength > 1000) {
      return userPlan === 'enterprise' ? 'claude-3' : 'gpt-4'
    }

    return 'gpt-3.5-turbo'
  }

  private analyzeComplexity(question: string): number {
    // ç®€å•çš„å¤æ‚åº¦åˆ†æ
    const complexKeywords = [
      'è®¾è®¡', 'åˆ†æ', 'è®¡ç®—', 'æ¨å¯¼', 'è¯æ˜', 'ä¼˜åŒ–',
      'design', 'analyze', 'calculate', 'derive', 'prove'
    ]
    
    const hasComplexKeywords = complexKeywords.some(keyword =>
      question.toLowerCase().includes(keyword)
    )
    
    const hasMultipleQuestions = (question.match(/[ï¼Ÿ?]/g) || []).length > 1
    const hasCodeBlocks = question.includes('```')
    
    let complexity = 0.3 // åŸºç¡€å¤æ‚åº¦
    
    if (hasComplexKeywords) complexity += 0.3
    if (hasMultipleQuestions) complexity += 0.2
    if (hasCodeBlocks) complexity += 0.2
    if (question.length > 500) complexity += 0.2
    
    return Math.min(complexity, 1.0)
  }
}
```

## ğŸ“Š ç›‘æ§å’Œåˆ†æ

### 1. AI ä½¿ç”¨ç»Ÿè®¡

```typescript
// apps/backend/src/ai/services/analytics.service.ts
import { Injectable } from '@nestjs/common'

@Injectable()
export class AIAnalyticsService {
  async recordInteraction(data: {
    userId: string
    question: string
    answer: string
    model: string
    tokensUsed: number
    responseTime: number
    confidence: number
    sources: number
    satisfied?: boolean
  }) {
    // è®°å½•åˆ°æ•°æ®åº“
    await this.prisma.aiInteraction.create({
      data: {
        userId: data.userId,
        question: data.question,
        answer: data.answer,
        model: data.model,
        tokensUsed: data.tokensUsed,
        responseTime: data.responseTime,
        confidence: data.confidence,
        sourcesCount: data.sources,
        satisfied: data.satisfied,
        timestamp: new Date()
      }
    })

    // æ›´æ–°å®æ—¶ç»Ÿè®¡
    await this.updateRealTimeStats(data)
  }

  private async updateRealTimeStats(data: any) {
    const today = new Date().toISOString().split('T')[0]
    
    await Promise.all([
      // æ€»è¯·æ±‚æ•°
      this.redis.incr(`stats:requests:${today}`),
      // æŒ‰æ¨¡å‹ç»Ÿè®¡
      this.redis.incr(`stats:model:${data.model}:${today}`),
      // å¹³å‡å“åº”æ—¶é—´
      this.redis.lpush(`stats:response_time:${today}`, data.responseTime),
      // å¹³å‡ç½®ä¿¡åº¦
      this.redis.lpush(`stats:confidence:${today}`, data.confidence)
    ])
  }

  async getDashboardStats(days: number = 7) {
    const stats = []
    
    for (let i = 0; i < days; i++) {
      const date = new Date()
      date.setDate(date.getDate() - i)
      const dateStr = date.toISOString().split('T')[0]
      
      const [requests, responseTimes, confidences] = await Promise.all([
        this.redis.get(`stats:requests:${dateStr}`),
        this.redis.lrange(`stats:response_time:${dateStr}`, 0, -1),
        this.redis.lrange(`stats:confidence:${dateStr}`, 0, -1)
      ])
      
      stats.push({
        date: dateStr,
        requests: parseInt(requests || '0'),
        avgResponseTime: responseTimes.length > 0 
          ? responseTimes.reduce((sum, time) => sum + parseFloat(time), 0) / responseTimes.length
          : 0,
        avgConfidence: confidences.length > 0
          ? confidences.reduce((sum, conf) => sum + parseFloat(conf), 0) / confidences.length
          : 0
      })
    }
    
    return stats.reverse()
  }
}
```

## ğŸ¯ æ€»ç»“

è¿™å¥— AI é›†æˆæ–¹æ¡ˆå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

1. **æ¶æ„å®Œæ•´**: RAG + LangChain + å‘é‡æ•°æ®åº“å®Œæ•´æ–¹æ¡ˆ
2. **æ€§èƒ½ä¼˜åŒ–**: æ™ºèƒ½ç¼“å­˜ + ç›¸ä¼¼é—®é¢˜åŒ¹é… + æµå¼å“åº”
3. **æˆæœ¬æ§åˆ¶**: ç”¨æˆ·é™é¢ + æ™ºèƒ½æ¨¡å‹é€‰æ‹© + ç»“æœç¼“å­˜
4. **ç”¨æˆ·ä½“éªŒ**: å®æ—¶æµå¼è¾“å‡º + æ‰“å­—æœºæ•ˆæœ + æ¥æºå±•ç¤º
5. **å¯æ‰©å±•æ€§**: æ¨¡å—åŒ–è®¾è®¡ + å¤šæ¨¡å‹æ”¯æŒ + æ’ä»¶æœºåˆ¶
6. **ç›‘æ§å®Œå–„**: ä½¿ç”¨ç»Ÿè®¡ + è´¨é‡åˆ†æ + æˆæœ¬è¿½è¸ª

é€‚ç”¨äºéœ€è¦é›†æˆ AI é—®ç­”ã€çŸ¥è¯†åº“æ£€ç´¢ã€æ™ºèƒ½åŠ©æ‰‹ç­‰åŠŸèƒ½çš„åº”ç”¨é¡¹ç›®ã€‚